{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f5d17-8a62-4852-9880-046c4747db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install python-dotenv\n",
    "!pip3 install llama-index\n",
    "!pip3 install nebula2-python\n",
    "!pip3 install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4a6fcad2491e01",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T14:28:30.111886470Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T17:59:23.259928361Z",
     "start_time": "2023-10-03T17:59:23.257044644Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3b8bd12154c08e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:41:21.503526202Z",
     "start_time": "2023-09-24T16:41:21.498641724Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) # logging.DEBUG for more verbose output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9156604a37a82c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef853d2d925b136f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install llama_index\n",
    "# !pip install langchain\n",
    "# !pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33154abe-79ae-4b9b-a895-ec5fbd0bac44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:41:47.128975369Z",
     "start_time": "2023-09-24T16:41:36.914500072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "WARNING:llama_index.indices.service_context:chunk_size_limit is deprecated, please specify chunk_size instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "\n",
    "from langchain import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# define LLM\n",
    "# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd2f91814325124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:42:41.182474154Z",
     "start_time": "2023-09-24T16:42:41.113202068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d57b86f41f29117a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:46:03.731584765Z",
     "start_time": "2023-09-24T16:46:02.178147018Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-ngql in ./venv/lib/python3.11/site-packages (0.7.5)\n",
      "Requirement already satisfied: nebula3-python in ./venv/lib/python3.11/site-packages (3.4.0)\n",
      "Requirement already satisfied: Jinja2 in ./venv/lib/python3.11/site-packages (from ipython-ngql) (3.1.2)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from ipython-ngql) (2.1.1)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in ./venv/lib/python3.11/site-packages (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in ./venv/lib/python3.11/site-packages (from nebula3-python) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in ./venv/lib/python3.11/site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in ./venv/lib/python3.11/site-packages (from nebula3-python) (2023.3.post1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./venv/lib/python3.11/site-packages (from httplib2>=0.20.0->nebula3-python) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from Jinja2->ipython-ngql) (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas->ipython-ngql) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->ipython-ngql) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.11/site-packages (from pandas->ipython-ngql) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Create a NebulaGraph cluster with:\n",
    "    # Option 0 for machines with Docker: `curl -fsSL nebula-up.siwei.io/install.sh | bash`\n",
    "    # Option 1 for Desktop: NebulaGraph Docker Extension https://hub.docker.com/extensions/weygu/nebulagraph-dd-ext\n",
    "\n",
    "# If not, create it with the following commands from NebulaGraph's console:\n",
    "    # CREATE SPACE llamaindex(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "    # :sleep 10;\n",
    "    # USE llamaindex;\n",
    "    # CREATE TAG entity(name string);\n",
    "    # CREATE EDGE relationship(relationship string);\n",
    "    # :sleep 10;\n",
    "    # CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "\n",
    "%pip install ipython-ngql nebula3-python\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"<password>\" # default is \"nebula\n",
    "os.environ['NEBULA_ADDRESS'] = \"0.0.0.0:9669\" # assumed we have NebulaGraph installed locally\n",
    "\n",
    "# space_name = \"llamaindex\"\n",
    "space_name = \"linkedin_v2\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"] # default, could be omit if create from an empty kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c3b615cf26dfa3cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:53:32.242287050Z",
     "start_time": "2023-09-24T16:53:32.234949142Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_store = NebulaGraphStore(space_name=space_name, edge_types=edge_types, rel_prop_names=rel_prop_names, tags=tags)\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c2a2156363d541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:54:14.656653195Z",
     "start_time": "2023-09-24T16:54:07.957549174Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932abcf8-f924-4c3b-8986-dc59d52a9e25",
   "metadata": {},
   "source": [
    "# Kilian LinkedIn MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18e23f-c484-4cb3-9d61-c3e46e6d3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SPACE linkedin_v2(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "# :sleep 10;\n",
    "# USE linkedin_v2;\n",
    "# CREATE TAG entity(name string);\n",
    "# CREATE EDGE relationship(relationship string);\n",
    "# :sleep 10;\n",
    "# CREATE TAG INDEX entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37228231-89b1-4f63-901a-4034c8b7c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**To: Wey (Siwei) Gu & Lynn Bender**\n",
      "\n",
      "\n",
      "Dear Lynn and Wey,\n",
      "\n",
      "I hope this message finds you in good spirits. As recent additions to my LinkedIn network, Iâ€™ve admired your contributions and wondered if we could embark on an exciting collaboration.\n",
      "\n",
      "**Motivation:**  \n",
      "While preparing to share a recent article on RAM-based GNNs - Amazon Researchers Introduce DistTG, I realized its relevance to my previous posts. The current LinkedIn design, however, doesn't facilitate easy referencing of such connected content.\n",
      "\n",
      "**Context:**  \n",
      "LinkedIn, in its current state, encourages fast-paced, buzzword-heavy content, often neglecting the depth and interconnectedness of knowledge. Rather than building on previous insights or referencing established knowledge (standing on the shoulders of giants), content risks becoming ephemeral and redundant.\n",
      "\n",
      "**Solution:**  \n",
      "It's time for platforms like LinkedIn, and potentially others like Facebook, Instagram, or Twitter, to evolve. Imagine a version-controlled system, akin to Git, manifesting as a knowledge graph on LinkedIn.\n",
      "\n",
      "**Proof:**  \n",
      "By harnessing graph theory, we can utilize hashtags as edges and topic embeddings as nodes. The post's content can serve as neighboring nodes, creating a knowledge-rich network. This approach facilitates better connectivity between topics and in-depth discussions.\n",
      "\n",
      "**Approach:**  \n",
      "I've created a video demonstration using the Nebula Graph, which ingests my LinkedIn posts, rendering a visual knowledge network. The underlying engine powers this visualization, and I've also prepared a static file deployment.\n",
      "\n",
      "**Collaboration Request:**  \n",
      "I am eager to explore this idea further with your expertise. I have a Python class ready that extracts user post content from the official LinkedIn API. However, I lack the partner status required to query member post content. Your association and insights could bridge this gap.\n",
      "\n",
      "The prospect of reimagining LinkedIn's UI and algorithm with your collaboration excites me. Together, we can bring depth, interconnectivity, and a renewed sense of purpose to content sharing.\n",
      "\n",
      "I await your thoughts.\n",
      "\n",
      "Warm regards,\n",
      "\n",
      "Kilian\n",
      "# 1\n",
      "\n",
      "\"Now, envisage if each of you decided to share these individual â€˜conversation countriesâ€™ in a global open-source network. You would be contributing to a collective knowledge pool, allowing peers worldwide to explore different questioning and prompting styles, and exposing them to a myriad of question-answer universes.\"  \n",
      "  \n",
      "I didn't read yet any comment based onÂ [#gpt](https://www.linkedin.com/feed/hashtag/?keywords=gpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7112910622266142721)Â where the actual conversation link was shared.  \n",
      "  \n",
      "Based on the idea: Be the change you want to see in the world (ref. Gandhi):  \n",
      "  \n",
      "  \n",
      "(please excuse my very bad english :D)  \n",
      "  \n",
      "I have honestly no idea what happens if more than one person decides to continue the conversation, but please feel free to be part of aÂ [#socialexperiment](https://www.linkedin.com/feed/hashtag/?keywords=socialexperiment&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7112910622266142721)Â and answer and share again and so furth ([#rippleeffect](https://www.linkedin.com/feed/hashtag/?keywords=rippleeffect&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7112910622266142721))\n",
      "\n",
      "\n",
      "## 1\n",
      "\n",
      "\n",
      "# 2\n",
      "\n",
      "If you're on the hunt for an e2e content-to-graph pipeline, ğŸ§ ğŸ’» consider diving intoÂ [#NebulaGraph](https://www.linkedin.com/feed/hashtag/?keywords=nebulagraph&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7111793580876169216)Â byÂ [VeSOFT](https://www.linkedin.com/company/vesoft/)Â [https://lnkd.in/efmJxXNn](https://lnkd.in/efmJxXNn)Â ! They harness the might ofÂ [#llamaIndex](https://www.linkedin.com/feed/hashtag/?keywords=llamaindex&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7111793580876169216)Â [https://lnkd.in/eACkax7K](https://lnkd.in/eACkax7K)Â ğŸš€ğŸ” Running into hurdles? Catch my firsthand experience here:Â [https://lnkd.in/ez6jMKG2](https://lnkd.in/ez6jMKG2)  \n",
      "  \n",
      "A massive shoutout to Wey Gu for shedding light onÂ [#RAG](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7111793580876169216)Â and cross-chunk embeddings interactions, which will be playing a pivotal role in shaping future architectures. ğŸŒğŸ’¡ For a deeper dive, check out my last post:Â [https://lnkd.in/e_hkq8Ji](https://lnkd.in/e_hkq8Ji)\n",
      "\n",
      "\n",
      "# 3\n",
      "After some discussions with colleagues & friends and diving into the videos attached, I've come to believe that a transformative approach is on the horizon for companies with more than 1000 employees. Regardless of the platform - be itÂ [#arangodb](https://www.linkedin.com/feed/hashtag/?keywords=arangodb&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001),Â [#neo4j](https://www.linkedin.com/feed/hashtag/?keywords=neo4j&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001), orÂ [#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)Â [#neptune](https://www.linkedin.com/feed/hashtag/?keywords=neptune&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)Â - the foundational strategy remains consistent:  \n",
      "  \n",
      "1. Data Lake | Data Warehouse | DBs: The bedrock of data storage and management.  \n",
      "  \n",
      "2. ELT | ETL: A robust digestion layer that seamlessly integrates the different levels of structure.  \n",
      "  \n",
      "3. Feature-Store: An essential layer I'd introduce between the digestion and graph database. This acts as a bridge for User Memory Retrieval, Suggestion Storage, and Version Control Implementation. Its primary role?  \n",
      "  \n",
      "--> To intuitively model the problem spaces of team members, ensuring a more personalized and efficient data interaction. <--  \n",
      "  \n",
      "4. Graph-DB: This maps data relationships, providing a comprehensive view of interconnected information. It can also be useful for rebalancing the embeddings-space in context.  \n",
      "Thanks toÂ John Chong Min Tan, see 34:30 ::Â [https://lnkd.in/eXtwjipz](https://lnkd.in/eXtwjipz)  \n",
      "  \n",
      "5. Service-Mesh (uhhh, the bad-word has been used ;P ): The final layer ensuring smooth communication between Developers & Data Engineers and Scientists & Analysts, as well as machine-to-machine interactions.  \n",
      "  \n",
      "In the future, one can also consider anÂ [#ai](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)Â [#gai](https://www.linkedin.com/feed/hashtag/?keywords=gai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)Â agent-pool-handler approach to leverage agent behavior among the respective contextually-connected services.  \n",
      "  \n",
      "Thanks toÂ Ben LackeyÂ Architecture Overview:Â [https://lnkd.in/eepxZaKz](https://lnkd.in/eepxZaKz)\n",
      "\n",
      "\n",
      "# 4\n",
      "\n",
      "That's an interesting take onÂ [#llms](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7099107218129571840)Â of Mr. Stephen Wolfram inÂ [https://lnkd.in/eQ9bDpBZ](https://lnkd.in/eQ9bDpBZ):  \n",
      "UseÂ [#llms](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7099107218129571840)Â models as models of the human brain (that's why we perceive the answers of LLM powered apps likeÂ [#gpt](https://www.linkedin.com/feed/hashtag/?keywords=gpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7099107218129571840)Â as so astonishing, because they manage to model the way we think and provide the correct extrapolation as the answer to our prompts (sometimes)).  \n",
      "Concretely speaking:  \n",
      "Will it be possible to determine neuroplasticity & synaptic architecture of an individual based on a their interactions with a LLM (IV: Kind of LLM interactions, DV: neuroplasticity & synaptic architecture)?  \n",
      "If anybody has some intel (papers, references etc.) on that or interpreted the video differently, let me know (I'm interested in that kind of research, check out:Â [https://lnkd.in/eCjANtZz](https://lnkd.in/eCjANtZz)).\n",
      "\n",
      "\n",
      "# 5 \n",
      "\n",
      "whisper api ofÂ [#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â speech to text is very powerful.  \n",
      "Check out my app:  \n",
      "[https://lnkd.in/eZpcgKHp](https://lnkd.in/eZpcgKHp)  \n",
      "if you want to try yourself. If you need a quick intro, check out the video.  \n",
      "Sorry for getting distracted aboutÂ [#startup](https://www.linkedin.com/feed/hashtag/?keywords=startup&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â ideas :D  \n",
      "&& the translation part comes in v2, for now just multilingual  \n",
      "  \n",
      "--> update: v2 is deployed, same link. Video vor v2:Â [https://lnkd.in/ecZ_a5uG](https://lnkd.in/ecZ_a5uG)  \n",
      "  \n",
      "[#gpt](https://www.linkedin.com/feed/hashtag/?keywords=gpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â [#whisper](https://www.linkedin.com/feed/hashtag/?keywords=whisper&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â [#openaichatgpt](https://www.linkedin.com/feed/hashtag/?keywords=openaichatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â [#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â [#api](https://www.linkedin.com/feed/hashtag/?keywords=api&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)Â [#ai](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\n",
      "\n",
      "\n",
      "# 6 \n",
      "\n",
      "ğŸŒ± A Greener Future: Exploring the Impact of Straw Houses on the Carbon Footprint  \n",
      "1. Short Summary:  \n",
      "With climate change at the forefront of global challenges, innovative solutions are needed more than ever. Building homes with straw could be one such solution, subtracting CO2 from the atmosphere rather than adding to it. This project explores the potential of straw houses to reduce the carbon footprint of the housing industry, considering factors like degradation over time and exponential growth in straw house construction.  \n",
      "2. See Video Below:  \n",
      "Want to understand the concept better? Watch this video fromÂ [Patagonia](https://www.linkedin.com/company/patagonia_2/)Â Founder Yvon Chouinard, providing insights into the idea of building with straw and its environmental benefits.Â [https://lnkd.in/egyVAKxq](https://lnkd.in/egyVAKxq)  \n",
      "3. See App to Dig Deeper:  \n",
      "Ready to explore simulations of such an impact? Dive into my dash app. Analyze global trends or specific countries, see the potential reductions, and even adjust the analysis based on different starting decades (i used 1950 as base scenario, meaning what could have happend if we started back then) ğŸ“ŠÂ [https://lnkd.in/ei3YYHGT](https://lnkd.in/ei3YYHGT)  \n",
      "4. See Math Explanation:  \n",
      "Curious about the mathematical models behind the analysis? Check out the detailed explanation, including the equations and assumptions used in the calculations. ğŸ§® View the math under the tab \"Math Explanation\" in same app. I used \"Our World in Data\" data:Â [https://github.com/owid](https://github.com/owid)Â . Heads up, straw like ice like all other solutions as of now only store temporarily, so i had to introduce a degradation of the reduction effect of building straw houses.Â   \n",
      "5. See GitHub Repo:  \n",
      "For fellow developers and data enthusiasts, the entire codebase is available onÂ [#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)Â :Â [https://lnkd.in/errqTTEr](https://lnkd.in/errqTTEr)  \n",
      "By embracing innovative construction methods, we can work together to flatten the CO2 curve and build a more sustainable future. Let's explore, analyze, and innovate! ğŸŒğŸ’¡  \n",
      "[#Sustainability](https://www.linkedin.com/feed/hashtag/?keywords=sustainability&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)Â [#ClimateChange](https://www.linkedin.com/feed/hashtag/?keywords=climatechange&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)Â [#Innovation](https://www.linkedin.com/feed/hashtag/?keywords=innovation&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)Â [#StrawHouses](https://www.linkedin.com/feed/hashtag/?keywords=strawhouses&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)Â [#DataAnalysis](https://www.linkedin.com/feed/hashtag/?keywords=dataanalysis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\n",
      "\n",
      "\n",
      "# 7 \n",
      "\n",
      "In 2020, I dedicated my BA thesis to delve deep into the realm of data-driven decisions in response to the climate crisis. The introduction to my thesis read:  \n",
      "  \n",
      "\"In response to the climate debate, data-driven decisions are increasingly relied upon in Europe. These technocratic decisions (as they are often called in a political context)(cf. Radaelli, 2017) are behind national sanctions like the ones in force in the German city Stuttgart for exceeding the threshold value for air pollution. Because of the all-englobing nature of the problem of climate change, nearly every sector (primary, secondary and tertiary sector) of the economy is touched. But do all stakeholders in these sectors have access to reliable data? Does any citizen, policy maker, or scientist in Europe have access to consistent and reliable data? Does the mayor of a small village in Germany have access to the same environmental data as a policy maker in the EU or any other mayor in the EU. These questions are addressed in the work at hand for the member states France and Germany. The accessibility of data is critically necessary for the social acceptance of the legislation of increasingly stringent environmental measures (cf. Carrete et al., 2012). This principle dovetails with one of the main principles of the Balanced-Scorecard-Topic: â€ If you canâ€™t measure it, you canâ€™t manage it. â€(Kaplan and David, 2000, p. 21) or in this context: you canâ€™t make effective managerial decisions without reliable access to consistent data.â€  \n",
      "  \n",
      "You can find the complete thesis here:Â [https://lnkd.in/eNMHySsr](https://lnkd.in/eNMHySsr)  \n",
      "  \n",
      "I'm gratified to see that innovative start-ups like Regionalwert Research:Â [https://lnkd.in/ek4eikJF](https://lnkd.in/ek4eikJF)Â andÂ the promising @IsometricÂ [https://lnkd.in/es_Raq82](https://lnkd.in/es_Raq82)Â are harnessing the power of data to make informed decisions. These organizations are paving the way to a future where data accessibility isn't a privilege but a norm.  \n",
      "  \n",
      "Let's empower everyone, from policy-makers to citizens, with the data they need to make informed decisions about our planet's future.  \n",
      "  \n",
      "[#DataDrivenDecisions](https://www.linkedin.com/feed/hashtag/?keywords=datadrivendecisions&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7087193892416012288)Â [#ClimateCrisis](https://www.linkedin.com/feed/hashtag/?keywords=climatecrisis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7087193892416012288)Â [#DataAccessibility](https://www.linkedin.com/feed/hashtag/?keywords=dataaccessibility&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7087193892416012288)\n",
      "\n",
      "\n",
      "# 8 \n",
      "\n",
      "ğŸš€Excited to share simple Approach for Unraveling the mystery of JSON extraction in BQ!ğŸ”  \n",
      "  \n",
      "â¡ï¸Coming up next: keep an eye out for our mini-series on Parameterized Queries, and getting hands-on with Dataform and Apache Beam!  \n",
      "  \n",
      "[#BigQuery](https://www.linkedin.com/feed/hashtag/?keywords=bigquery&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)Â [#GoogleCloud](https://www.linkedin.com/feed/hashtag/?keywords=googlecloud&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)Â [#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)Â [#Dataform](https://www.linkedin.com/feed/hashtag/?keywords=dataform&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)Â [#ApacheBeam](https://www.linkedin.com/feed/hashtag/?keywords=apachebeam&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)Â [#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)Â [#TutorialSeries](https://www.linkedin.com/feed/hashtag/?keywords=tutorialseries&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\n",
      "\n",
      "\n",
      "# 9 \n",
      "\n",
      "Delighted to share a resource that could potentially help all those who still shudder at the mention of high school math. Here's a video that might just be the remedy you need:  \n",
      "ğŸ¥ Animated Math Tutorial - Kudos to @alanbecker for making math more approachable and fun!Â [https://lnkd.in/ext6wHDN](https://lnkd.in/ext6wHDN)  \n",
      "And for those of you who are looking to dive deeper into the analogies and explore a deeper understanding, I highly recommend watching the following 'lyrics' video:  \n",
      "ğŸ¥ Math Analogy Explained - Big shoutout to @prosperpython for their insightful commentary!Â [https://lnkd.in/epEyePxE](https://lnkd.in/epEyePxE)  \n",
      "Take a moment out of your busy schedules to appreciate the beauty of math explained in an engaging and enjoyable manner. Happy learning! ğŸ“š  \n",
      "[#MathMadeEasy](https://www.linkedin.com/feed/hashtag/?keywords=mathmadeeasy&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)Â [#LearningResources](https://www.linkedin.com/feed/hashtag/?keywords=learningresources&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)Â [#AlanBecker](https://www.linkedin.com/feed/hashtag/?keywords=alanbecker&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)Â [#ProsperPython](https://www.linkedin.com/feed/hashtag/?keywords=prosperpython&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)Â [#Mathematics](https://www.linkedin.com/feed/hashtag/?keywords=mathematics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)\n",
      "\n",
      "\n",
      "# 10 \n",
      "\n",
      "I beg your indulgence as I once again delve into the delightful realm of analogy-laden musings (honestly, i have the best time and i guess you guys have to suffer :D ):  \n",
      "  \n",
      "ğŸ¬ Like the characters in 'Inception' and 'Predestination', I navigate through layers of reality - only, my layers are not dreams or timelines, but data-sets and tables in DBs. In our latest project, I've designed a class, which weaves through these data-scapes, building views, fetching the latest tables, and generating summaries for reports.  \n",
      "  \n",
      "Our raw data reports remain static, a frozen snapshot of a moment in time. Just as in quantum theory, where a particle's state isn't determined until it's observed (principle of superposition), our raw data only unfolds its patterns and trends when the view is invoked.  \n",
      "  \n",
      "Just as a clockmaker peers into the intricate labyrinths of cogs and springs, refining their dance within the heart of a timepiece, I too navigate the maze of data in the realm of temporal dimensions.  \n",
      "  \n",
      "Or just as the agents in '[#inception](https://www.linkedin.com/feed/hashtag/?keywords=inception&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)' create their dream world, my class generates temporary tables from these views, crafting a unique vision of the data landscape.  \n",
      "It's like navigating through a world that is constantly being rewritten, much like how the time agent's actions constantly reshaped his timeline in 'Predestination'(Prudhvee Krishna Nalluri, thx. mate!ğŸ‘ŒğŸ¥).  \n",
      "He voyages through the weft and weave of time to thwart calamities before they occur. Similar to him, I delve into the intricacies of the most recent data tables to identify and mitigate any potential issues before they occur.  \n",
      "  \n",
      "In the end, we form a dance between creation and destruction, viewing and refreshing, much like the ebb and flow of time in our sci-fi thrillers. It's an intricate game that transforms raw, unobserved data into rich, meaningful insights, ready to drive decision-making in this ever-evolving data-driven world.  \n",
      "  \n",
      "[#BigQuery](https://www.linkedin.com/feed/hashtag/?keywords=bigquery&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)Â [#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)Â [#Python](https://www.linkedin.com/feed/hashtag/?keywords=python&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)Â [#SciFiMetaphors](https://www.linkedin.com/feed/hashtag/?keywords=scifimetaphors&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)Â [#Inception](https://www.linkedin.com/feed/hashtag/?keywords=inception&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)Â [#Predestination](https://www.linkedin.com/feed/hashtag/?keywords=predestination&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\n",
      "\n",
      "\n",
      "# 11\n",
      "\n",
      "Hello Weekend-Hustlers! ğŸŒ  \n",
      "Straight from the heart of the Four Corners of Civilization, I'm going to share an opinionated process-approach that's been my Fae companion in the treacherous journey through the coding Edema Ruh troupe.  \n",
      "Use-Case: Think of projects that resemble the intricate harmony between the Arcanum and the University, where the silent communication between an Artificer and their Kilvin is crucial, and our Sympathy links with monitoring and testing are our saving grace.  \n",
      "Consider the following as a pseudo-code class: TheKingKillerChronicle. This is an epic saga, not unlike Kvothe's own tale, where each class and function is a character. The story revolves around dancing with the ever-lurking Cthaeh, AKA Conway's Law, without being bitten.  \n",
      "Class TheKingKillerChronicle inherits from RuhLore:  \n",
      "ğŸ”¹ Method 1 - Design Workshop: Like young Kvothe learning the Edema Ruh's ways, our Sympathist class acquires domain-specific terms from the stakeholder. Learning outcomes, SLAs, and value errors are etched in the Waystone Inn's ledger, in our case, UML. This is where our tale begins, a common tongue is agreed upon, and we lay the groundwork for our defense against the Cthaeh's prophecies.  \n",
      "ğŸ”¹ Method 2 - Infrastructure as Code (IaC): Picture this as Elodin's Naming class, granting first-class citizenship to your infrastructure in the realm of code. Whether it's the primal essence of Terraform or the syntactic consistency of CloudFormation, your infrastructure will hum along like the wind called by a namer.  \n",
      "ğŸ”¹ Method 3 - Meta-Decorator: This is like a more versatile version of Mola's careful administrations in the Medica - a log_and_test() function on a potent alchemical mixture. From translating SLAs into tangible elements and monitoring them like the Masters watch the Crockery, to configuring errors and redundancies, this method is as meticulous as a well-performed Binding.  \n",
      "ğŸ”¹ Method 4 - Architecture Feedback Loop: The grand finale, the closing notes of Kvothe's lute at the Eolian. We convene with stakeholders, our resolution map in hand, and hold it up to the golden standard set in the Design Workshop. With print(help(ClassOne)) as our lens, we refine it until it becomes a song that will echo through the centuries.  \n",
      "Check out the attached UML diagram for a glimpse into our saga, in a form even the Archives couldn't misplace.  \n",
      "In the end, the most skilled Shapers aren't just those fluent in the arcane languages of code, but also those adept in the arts of communication, planning, and feedback. As they say in the Commonwealth, may all your stories be glad ones! ğŸ»  \n",
      "[#CodingLife](https://www.linkedin.com/feed/hashtag/?keywords=codinglife&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)Â [#SoftwareDesign](https://www.linkedin.com/feed/hashtag/?keywords=softwaredesign&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)Â [#WeekendHustle](https://www.linkedin.com/feed/hashtag/?keywords=weekendhustle&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)Â [#DevOps](https://www.linkedin.com/feed/hashtag/?keywords=devops&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)Â [#ContinuousLearning](https://www.linkedin.com/feed/hashtag/?keywords=continuouslearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)\n",
      "\n",
      "\n",
      "\n",
      "# 12\n",
      "\n",
      "[#attentionIsAllYouNeed](https://www.linkedin.com/feed/hashtag/?keywords=attentionisallyouneed&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7071455186656874496)Â thanksÂ [Joshua Starmer PhD](https://www.linkedin.com/in/ACoAACAd8VYBqvN-leU7VPrO93tKYbEfzhAFOgY)  \n",
      "[https://lnkd.in/evk3t5G3](https://lnkd.in/evk3t5G3)  \n",
      "[#triplebam](https://www.linkedin.com/feed/hashtag/?keywords=triplebam&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7071455186656874496)  \n",
      "[https://lnkd.in/erFSNUym](https://lnkd.in/erFSNUym)\n",
      "\n",
      "\n",
      "\n",
      "# 13\n",
      "\n",
      "Of many nice frameworks forÂ [#graph](https://www.linkedin.com/feed/hashtag/?keywords=graph&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177)Â [#visualization](https://www.linkedin.com/feed/hashtag/?keywords=visualization&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177), there is one which is for me at the absolute top:Â [https://threejs.org/](https://threejs.org/)Â and one dev who pushes its boundaries: Andrei Kashcha (big-up!), i can only hope that i reach one day his level:  \n",
      "[https://lnkd.in/evRgE_Tv](https://lnkd.in/evRgE_Tv)  \n",
      "(please, don't forget to zoom in)  \n",
      "Also nice to use:  \n",
      "1.Â [https://d3js.org/](https://d3js.org/)  \n",
      "2.Â [https://viz-js.com/](https://viz-js.com/)  \n",
      "3.Â [https://lnkd.in/eFMGJRHq](https://lnkd.in/eFMGJRHq)  \n",
      "  \n",
      "And here for my academic acquaintances:  \n",
      "[https://lnkd.in/eimR3Und](https://lnkd.in/eimR3Und)  \n",
      "  \n",
      "And for those who ever wanted to seeÂ [#rstats](https://www.linkedin.com/feed/hashtag/?keywords=rstats&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177)Â andÂ [#python](https://www.linkedin.com/feed/hashtag/?keywords=python&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177)Â packages and their dependencies visualized:  \n",
      "[https://lnkd.in/ei-yJhPB](https://lnkd.in/ei-yJhPB)\n",
      "\n",
      "\n",
      "# 14\n",
      "\n",
      "For anyone who didnt hear the news,Â [#vertexai](https://www.linkedin.com/feed/hashtag/?keywords=vertexai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7062178683528060928)Â drawn equal withÂ [#sagemaker](https://www.linkedin.com/feed/hashtag/?keywords=sagemaker&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7062178683528060928)Â regardingÂ [#llms](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7062178683528060928)Â [functionality-offer wise]\n",
      "\n",
      "\n",
      "# 15 \n",
      "\n",
      "Mind following aspects when starting training & fine tuningÂ [#LLMs](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008):  \n",
      "1. Don't waste time and money and try and run aÂ [#LLM](https://www.linkedin.com/feed/hashtag/?keywords=llm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008)Â on a notebook, you will get cuda out of memory errors all the time, because the cuda driver doesnt know how to handle multiple notebooks or even if you open a console etc. it will have problems. And dont get me started on server disconnects :D  \n",
      "2. If you go for an instance, go for the g-family inÂ [#ec2](https://www.linkedin.com/feed/hashtag/?keywords=ec2&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008), the p-family gave me problems with theÂ [#cuda](https://www.linkedin.com/feed/hashtag/?keywords=cuda&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008)Â driver  \n",
      "3. Start to understand the connection between mirco-batches, RAM and learning-rate, if not your convergance gonna mess up your results  \n",
      "4. Start to parallelize your LLM training and fine tuning, espacially if you run one more than one gpu, if not your money will be waisted on the other gpus.\n",
      "\n",
      "\n",
      "# 16 \n",
      "\n",
      "\n",
      "I fine tuned like a dozenÂ [#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7046189214668836864)Â models (ada, davinci, curie etc.), in all of them i got:  \n",
      "  \n",
      "Job complete! Status: succeeded ğŸ‰  \n",
      "... but one:  \n",
      "Job complete! Status: succeeded Ã°ï¿½ï¿½ï¿½  \n",
      "  \n",
      "Anyone any idea what Eth ( Ã°) is doing there? :D  \n",
      "  \n",
      "One further note here:  \n",
      "You can further fine tune your already fine tuned models. So you can basically fine tune model a for agency a and model b for agency b and then you make model c based on agency model a and b and so you can build your company-knowledge-[#ai](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7046189214668836864)Â in a layered logic.  \n",
      "  \n",
      "And here a small batch script so you're not stuck in a queue while fine tuning:D  \n",
      "  \n",
      "#!/usr/bin/expect -f  \n",
      "  \n",
      "# set the timeout to 1 second  \n",
      "set timeout 1  \n",
      "  \n",
      "# define the command to be run  \n",
      "set COMMAND \"openai api fine_tunes.follow -i ft-XXX\"  \n",
      "  \n",
      "# spawn a new shell and start the command  \n",
      "spawn sh  \n",
      "  \n",
      "send \"$COMMAND\\n\"  \n",
      "  \n",
      "# continuously monitor the console output for \"disconnect\"  \n",
      "while true {  \n",
      "expect {  \n",
      "-re \".*disconnect.*\" {  \n",
      "# if \"disconnect\" is found, run the command again  \n",
      "puts \"Connection lost! Reconnecting...\"  \n",
      "send \"$COMMAND\\n\"  \n",
      "}  \n",
      "timeout {  \n",
      "# wait for 1 second before checking again  \n",
      "sleep 1  \n",
      "}  \n",
      "}  \n",
      "}\n",
      "\n",
      "\n",
      "# 18 \n",
      "\n",
      "Happy birthdayÂ [#patagonia](https://www.linkedin.com/feed/hashtag/?keywords=patagonia&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7042849455431139328)Â [#responsibility](https://www.linkedin.com/feed/hashtag/?keywords=responsibility&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7042849455431139328)\n",
      "\n",
      "\n",
      "# 19 \n",
      "\n",
      "For all of you who had to do hard string operations with high server costs in the past. These days are over, thanks toÂ [#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040370029950873601)  \n",
      "Go for Fine-Tuning:Â [https://lnkd.in/dK2jWTgr](https://lnkd.in/dK2jWTgr)  \n",
      "  \n",
      "Your models will appear afterwards in the playground and you can use them in your apis.  \n",
      "  \n",
      "Pitfalls at the moment:  \n",
      "1. Could solve nor find solution for following error:  \n",
      "'The number of classes in file does not match the number of classes specified in the hyperparameters.'  \n",
      "2. You will have a lot of server disconnect messages and you will be stuck in a queu for a few minutes\n",
      "\n",
      "\n",
      "\n",
      "# 20 \n",
      "\n",
      "Hi, I have developed an app based onÂ [REWE](https://www.linkedin.com/company/rewe/)Â [REWE Group](https://www.linkedin.com/company/rewegroup/)Â E-Bon, because unfortunately you can only get it as a pdf via the rewe app and only aggergated by purchase trip as csv, but I wanted to have the listing of products via csv. You can use the app to convert the receipt into a csv and if you want to do me something good, you can also provide it to me. Mind that it's still in Beta. More here:  \n",
      "  \n",
      "[https://lnkd.in/e2fBFmsR](https://lnkd.in/e2fBFmsR)\n",
      "\n",
      "\n",
      "\n",
      "# 21\n",
      "\n",
      "The beauty of layering APIs ([#chatgpt](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)Â [#wolfram](https://www.linkedin.com/feed/hashtag/?keywords=wolfram&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)Â [#whisper](https://www.linkedin.com/feed/hashtag/?keywords=whisper&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)) each one playing on it's strength &Â [#huggingface](https://www.linkedin.com/feed/hashtag/?keywords=huggingface&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)Â has direct integration toÂ [#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)Â [#sagemaker](https://www.linkedin.com/feed/hashtag/?keywords=sagemaker&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)  \n",
      "Game is on forÂ [#AI_Composing](https://www.linkedin.com/feed/hashtag/?keywords=ai_composing&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)\n",
      "\n",
      "\n",
      "# 22\n",
      "Want to get started with applied reinforcement learning?  \n",
      "First get gym.  \n",
      "Following tutorial will help you to set it up correctly:  \n",
      "[https://lnkd.in/emyPPUaf](https://lnkd.in/emyPPUaf)  \n",
      "[Dibya Chakravorty](https://www.linkedin.com/in/ACoAABmJ5cgBCVP-7JVVogFK7flj2iVNyuedHP4)Â thx. mate :)\n",
      "\n",
      "\n",
      "# 23\n",
      "\n",
      "New use-case forÂ [#chatgpt](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)Â you can map all kinds ofÂ [#networks](https://www.linkedin.com/feed/hashtag/?keywords=networks&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)Â from your cdrive to your companyÂ [#sharepoint](https://www.linkedin.com/feed/hashtag/?keywords=sharepoint&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344), use this repo:Â [https://lnkd.in/gT2cznKj](https://lnkd.in/gT2cznKj)Â and if you already haveÂ [#neo4j](https://www.linkedin.com/feed/hashtag/?keywords=neo4j&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)Â , you can compare your indexing against the oneÂ [#chatgpt](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)Â would apply :D\n",
      "\n",
      "\n",
      "# 24\n",
      "\n",
      "[#gcp](https://www.linkedin.com/feed/hashtag/?keywords=gcp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7025949249414590465)Â [#bigquery](https://www.linkedin.com/feed/hashtag/?keywords=bigquery&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7025949249414590465)Â has now the feature lineage and execution graph: In case you wonder how to improve your query and make it cost efficient\n",
      "\n",
      "\n",
      "# Just read an article that could provide a real-time feedback-loop not only for logistic-models but also for retailer-models, imagine, how much faster the iterations would run through ::Â [https://lnkd.in/g9_vbF3F](https://lnkd.in/g9_vbF3F)Â [#rewe](https://www.linkedin.com/feed/hashtag/?keywords=rewe&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([REWE](https://www.linkedin.com/company/rewe/),Â [REWE Group](https://www.linkedin.com/company/rewegroup/)),Â [#lidl](https://www.linkedin.com/feed/hashtag/?keywords=lidl&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([Lidl in Germany](https://www.linkedin.com/company/lidl-in-deutschland/),Â [Lidl Portugal](https://www.linkedin.com/company/lidlportugal/)),Â [#aldi](https://www.linkedin.com/feed/hashtag/?keywords=aldi&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([ALDI SÃœD](https://www.linkedin.com/company/aldi-sued/),Â [ALDI USA](https://www.linkedin.com/company/aldi-usa/)),Â [#penny](https://www.linkedin.com/feed/hashtag/?keywords=penny&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072),Â [#netto](https://www.linkedin.com/feed/hashtag/?keywords=netto&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([Netto Marken-Discount](https://www.linkedin.com/company/netto-marken-discount/),Â [Netto](https://www.linkedin.com/company/netto/)),Â [#kaufland](https://www.linkedin.com/feed/hashtag/?keywords=kaufland&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([Kaufland](https://www.linkedin.com/company/kaufland-deutschland/),Â [Kaufland e-commerce](https://www.linkedin.com/company/kauflandecommerce/)),Â [#real](https://www.linkedin.com/feed/hashtag/?keywords=real&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ,Â [#edeka](https://www.linkedin.com/feed/hashtag/?keywords=edeka&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([EDEKA](https://www.linkedin.com/company/edeka/)),Â [#dm](https://www.linkedin.com/feed/hashtag/?keywords=dm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072),Â [#denns](https://www.linkedin.com/feed/hashtag/?keywords=denns&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([Denns BioMarkt](https://www.linkedin.com/company/denns-biomarkt-gmbh/)Â ),Â [#alnatura](https://www.linkedin.com/feed/hashtag/?keywords=alnatura&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)Â ([Alnatura](https://www.linkedin.com/company/alnatura/))\n",
      "\n",
      "\n",
      "\n",
      "# 25 \n",
      "\n",
      "For all those out there who ever wondered whatÂ [#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6999448747759783937)Â is, here is a very fresh intro ofÂ [#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6999448747759783937)Â itself.  \n",
      "[#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6999448747759783937)Â [GitHub](https://www.linkedin.com/company/github/)Â guys, thanks for everything.\n",
      "\n",
      "# 26 \n",
      "\n",
      "maybe interesting for anyone who doesÂ [#onlinebanking](https://www.linkedin.com/feed/hashtag/?keywords=onlinebanking&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6996584105404420096)Â :  \n",
      "I belive, that a product is as good as itsÂ [#community](https://www.linkedin.com/feed/hashtag/?keywords=community&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6996584105404420096)Â (e.g. look atÂ [#battelfield](https://www.linkedin.com/feed/hashtag/?keywords=battelfield&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6996584105404420096)Â 2042 ;D).  \n",
      "Thus, as a customer ofÂ [comdirect â€“ eine Marke der Commerzbank AG](https://www.linkedin.com/company/comdirect-bank/)Â I'm grateful, that you have an api (seeÂ [https://lnkd.in/e7jp4WwV](https://lnkd.in/e7jp4WwV)).  \n",
      "  \n",
      "But would you pls. make a read-only / reporting access with the chance to auto auth. instead of just giving the chance to make a read & WRITE access to a private online account? :D  \n",
      "As a banchmark you can takeÂ [Wise](https://www.linkedin.com/company/wiseaccount/), they did it right ;P  \n",
      "  \n",
      "--> Updt.: Found a nice workaround: useÂ [https://lnkd.in/eqsVzZrR](https://lnkd.in/eqsVzZrR)Â , they use the api nicely and we're out of the risky \"own dev\" scope.\n",
      "\n",
      "\n",
      "# 27 \n",
      "\n",
      "Here's aÂ [#startup](https://www.linkedin.com/feed/hashtag/?keywords=startup&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040)Â idea: using AI,Â [#DALLÂ·E](https://www.linkedin.com/feed/hashtag/?keywords=dall%C2%B7e&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040)Â or ratherÂ [#DALLÂ·Es](https://www.linkedin.com/feed/hashtag/?keywords=dall%C2%B7es&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040)Â interpretation (in stats terms: the noise, playing around with the schedule) as mediator between sides (whatever they may be). Here's a beautiful example that kept me awake the whole night:  \n",
      "  \n",
      "I askedÂ [#DALLÂ·E](https://www.linkedin.com/feed/hashtag/?keywords=dall%C2%B7e&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040):  \n",
      "\"the discours between the Standard Model of Particle Physics with the subatomic particle physics interpreted out of the Langlands program perspective of unitingÂ number theory and geometry represented in a graphical color mix\"  \n",
      "  \n",
      "Anybody up for it? ;P\n",
      "\n",
      "\n",
      "# 28 \n",
      "\n",
      "Hi guys,  \n",
      "just a quick heads up of for those interested inÂ [#spark](https://www.linkedin.com/feed/hashtag/?keywords=spark&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)Â [#r](https://www.linkedin.com/feed/hashtag/?keywords=r&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)Â [#python](https://www.linkedin.com/feed/hashtag/?keywords=python&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)Â :: TheÂ [#apachespark](https://www.linkedin.com/feed/hashtag/?keywords=apachespark&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)Â team slowly extends the documentation forÂ [#r](https://www.linkedin.com/feed/hashtag/?keywords=r&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)Â (e.g. One-vs-All missing in .r). Great to see that the bilinguality in the statistical world evolves ; * ::\n",
      "\n",
      "\n",
      "# 29 \n",
      "\n",
      "In the off chance, that you work with x>2 TB in pyspark (look below), if not, stop reading HERE, be happy and enjoy the summer :)  \n",
      "Â   \n",
      "1. Use zeppelin notebook instead of jupyter (if you don't believe me, try to cache in a jupyter nb. More then 2 Tb, if you manage, write me)  \n",
      "  \n",
      "2. Use highmem instances if you work on gcp (mind your budget)  \n",
      "  \n",
      "3. Set memory config to x>64g (if you have enough worker nodes), if not, you get a buffer overflow (especially if you do string ops.) [thx. to exodus-intelligence to put me on the topic]  \n",
      "  \n",
      "4. Cache() the data you want to work with (cache and sample, then checkpoint by saving to bucket, then read in and cache again in case you want to make some fancy visualization)  \n",
      "  \n",
      "5. Before you start with any modelling at all, try to reduce the dimensions (your columns/ variables) as far as possible (use pairplot to get a first idea, then go for pca, plot dimensions as overlay to datapoints)  \n",
      "Â   \n",
      "Thx. To FN, SG, InS for those insights.  \n",
      "Â   \n",
      "P.S. If there is anybody out there, who managed to run sparklyr on a zeppelin notebook, pls. Write me, I will buy you a bottle of wine (or juice).\n",
      "\n",
      "\n",
      "# 30 \n",
      "\n",
      "Knowledge is sth. beautiful, but as long as we don't structure it, it's ambiguous, and misunderstandins are prone to happen.  \n",
      "  \n",
      "I wrote a function in .r (kind of a parser), which translates my former knowledge-DB (.xml) to obsidian (auto generate the .md). Check out the beauty of it ;P  \n",
      "  \n",
      "For the future I want to integrateÂ [#rstudio](https://www.linkedin.com/feed/hashtag/?keywords=rstudio&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)Â ,Â [#wiki](https://www.linkedin.com/feed/hashtag/?keywords=wiki&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)Â .js &Â [#obsidian](https://www.linkedin.com/feed/hashtag/?keywords=obsidian&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)Â (and as a long-shot theÂ [#termstore](https://www.linkedin.com/feed/hashtag/?keywords=termstore&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)Â ofÂ [#sharepoint](https://www.linkedin.com/feed/hashtag/?keywords=sharepoint&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)Â ), to bring the best of all worlds together and stcuture my scripts & data & meta-data in a new way. Somebody any experience with that integration?\n",
      "\n",
      "\n",
      "# 31\n",
      "\n",
      "For those who ever wondered why theÂ [#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6899827928369127424)Â documentation:Â [https://lnkd.in/gKS6K3Qb](https://lnkd.in/gKS6K3Qb)Â regarding setting up an EMR R-Studio spark cluster doesn't work for you, don't worry, it's not your fault.Â [#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6899827928369127424)Â doesn't care to keep the documentation updated! I'm happy to supply you with my build file:  \n",
      "[https://lnkd.in/gjR3iscZ](https://lnkd.in/gjR3iscZ)Â (Remember to change default PW!)  \n",
      "One other thing,Â [#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6899827928369127424)Â also doesn't allocate all worker nodes to spark, you need to use:  \n",
      "[ { \"Classification\": \"spark\", \"Properties\": { \"maximizeResourceAllocation\": \"true\" } } ]\n",
      "\n",
      "\n",
      "# 32\n",
      "\n",
      "For all of those who loveÂ [#rshiny](https://www.linkedin.com/feed/hashtag/?keywords=rshiny&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â and want to asses their dev-approach or just want to get rid of the spaghetti bowl (Eric Nantz - Ian Lyttle) here comes a smooth approach: Use reactlogÂ [https://lnkd.in/guBCi79a](https://lnkd.in/guBCi79a)  \n",
      "  \n",
      "While doing one of my rare posts:  \n",
      "This post helps to get in toÂ [#nlp](https://www.linkedin.com/feed/hashtag/?keywords=nlp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â [https://lnkd.in/guaiJH8v](https://lnkd.in/guaiJH8v)  \n",
      "Â   \n",
      "But for those searching for the right library to visÂ [#nlp](https://www.linkedin.com/feed/hashtag/?keywords=nlp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â [#visualization](https://www.linkedin.com/feed/hashtag/?keywords=visualization&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â [#interactivly](https://www.linkedin.com/feed/hashtag/?keywords=interactivly&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112), cheers to \"networkD3\"Â [https://lnkd.in/grjVXbqM](https://lnkd.in/grjVXbqM)Â - great job guys!  \n",
      "Â   \n",
      "Use Case:Â thx. ToÂ [#PetitBiscuit](https://www.linkedin.com/feed/hashtag/?keywords=petitbiscuit&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â - We Were Young ft. JP Cooper (Lyrics)Â Â [https://lnkd.in/gX6ndzeR](https://lnkd.in/gX6ndzeR)Â for making good music andÂ [#youtube](https://www.linkedin.com/feed/hashtag/?keywords=youtube&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â for providing transcripts (I have to benchmark it next time againstÂ [#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)Â transcriber andÂ [#gcp](https://www.linkedin.com/feed/hashtag/?keywords=gcp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\n"
     ]
    }
   ],
   "source": [
    "def read_md_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "md_content = read_md_file('/home/kilian/google-drive/knowledgedb/231003/231003_linkedIn_posts.md')\n",
    "print(md_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c25257a-4bbe-4c58-980f-88ae7d6165a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "# Convert the custom text into the desired format\n",
    "document2 = Document(id=\"custom_doc_1\", text=md_content, metadata={\"source\": \"custom\"})\n",
    "\n",
    "print(type(document2))  # This should output <class 'llama_index.schema.Document'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0a3f8f1-7946-403f-91e8-b6dded3d5159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='8f160064-d6e2-4ccb-a0c6-6197e4dc7f24', embedding=None, metadata={'source': 'custom'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3e5b48ab9c3c8d9eb3b2a475e3cebbcb4a0719d54ebf231b6ae37fcd83dbc755', text='**To: Wey (Siwei) Gu & Lynn Bender**\\n\\n\\nDear Lynn and Wey,\\n\\nI hope this message finds you in good spirits. As recent additions to my LinkedIn network, Iâ€™ve admired your contributions and wondered if we could embark on an exciting collaboration.\\n\\n**Motivation:**  \\nWhile preparing to share a recent article on RAM-based GNNs - Amazon Researchers Introduce DistTG, I realized its relevance to my previous posts. The current LinkedIn design, however, doesn\\'t facilitate easy referencing of such connected content.\\n\\n**Context:**  \\nLinkedIn, in its current state, encourages fast-paced, buzzword-heavy content, often neglecting the depth and interconnectedness of knowledge. Rather than building on previous insights or referencing established knowledge (standing on the shoulders of giants), content risks becoming ephemeral and redundant.\\n\\n**Solution:**  \\nIt\\'s time for platforms like LinkedIn, and potentially others like Facebook, Instagram, or Twitter, to evolve. Imagine a version-controlled system, akin to Git, manifesting as a knowledge graph on LinkedIn.\\n\\n**Proof:**  \\nBy harnessing graph theory, we can utilize hashtags as edges and topic embeddings as nodes. The post\\'s content can serve as neighboring nodes, creating a knowledge-rich network. This approach facilitates better connectivity between topics and in-depth discussions.\\n\\n**Approach:**  \\nI\\'ve created a video demonstration using the Nebula Graph, which ingests my LinkedIn posts, rendering a visual knowledge network. The underlying engine powers this visualization, and I\\'ve also prepared a static file deployment.\\n\\n**Collaboration Request:**  \\nI am eager to explore this idea further with your expertise. I have a Python class ready that extracts user post content from the official LinkedIn API. However, I lack the partner status required to query member post content. Your association and insights could bridge this gap.\\n\\nThe prospect of reimagining LinkedIn\\'s UI and algorithm with your collaboration excites me. Together, we can bring depth, interconnectivity, and a renewed sense of purpose to content sharing.\\n\\nI await your thoughts.\\n\\nWarm regards,\\n\\nKilian\\n# 1\\n\\n\"Now, envisage if each of you decided to share these individual â€˜conversation countriesâ€™ in a global open-source network. You would be contributing to a collective knowledge pool, allowing peers worldwide to explore different questioning and prompting styles, and exposing them to a myriad of question-answer universes.\"  \\n  \\nI didn\\'t read yet any comment based on\\xa0[#gpt](https://www.linkedin.com/feed/hashtag/?keywords=gpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7112910622266142721)\\xa0where the actual conversation link was shared.  \\n  \\nBased on the idea: Be the change you want to see in the world (ref. Gandhi):  \\n  \\n  \\n(please excuse my very bad english :D)  \\n  \\nI have honestly no idea what happens if more than one person decides to continue the conversation, but please feel free to be part of a\\xa0[#socialexperiment](https://www.linkedin.com/feed/hashtag/?keywords=socialexperiment&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7112910622266142721)\\xa0and answer and share again and so furth ([#rippleeffect](https://www.linkedin.com/feed/hashtag/?keywords=rippleeffect&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7112910622266142721))\\n\\n\\n## 1\\n\\n\\n# 2\\n\\nIf you\\'re on the hunt for an e2e content-to-graph pipeline, ğŸ§ ğŸ’» consider diving into\\xa0[#NebulaGraph](https://www.linkedin.com/feed/hashtag/?keywords=nebulagraph&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7111793580876169216)\\xa0by\\xa0[VeSOFT](https://www.linkedin.com/company/vesoft/)\\xa0[https://lnkd.in/efmJxXNn](https://lnkd.in/efmJxXNn)\\xa0! They harness the might of\\xa0[#llamaIndex](https://www.linkedin.com/feed/hashtag/?keywords=llamaindex&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7111793580876169216)\\xa0[https://lnkd.in/eACkax7K](https://lnkd.in/eACkax7K)\\xa0ğŸš€ğŸ” Running into hurdles? Catch my firsthand experience here:\\xa0[https://lnkd.in/ez6jMKG2](https://lnkd.in/ez6jMKG2)  \\n  \\nA massive shoutout to Wey Gu for shedding light on\\xa0[#RAG](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7111793580876169216)\\xa0and cross-chunk embeddings interactions, which will be playing a pivotal role in shaping future architectures. ğŸŒğŸ’¡ For a deeper dive, check out my last post:\\xa0[https://lnkd.in/e_hkq8Ji](https://lnkd.in/e_hkq8Ji)\\n\\n\\n# 3\\nAfter some discussions with colleagues & friends and diving into the videos attached, I\\'ve come to believe that a transformative approach is on the horizon for companies with more than 1000 employees. Regardless of the platform - be it\\xa0[#arangodb](https://www.linkedin.com/feed/hashtag/?keywords=arangodb&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001),\\xa0[#neo4j](https://www.linkedin.com/feed/hashtag/?keywords=neo4j&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001), or\\xa0[#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)\\xa0[#neptune](https://www.linkedin.com/feed/hashtag/?keywords=neptune&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)\\xa0- the foundational strategy remains consistent:  \\n  \\n1. Data Lake | Data Warehouse | DBs: The bedrock of data storage and management.  \\n  \\n2. ELT | ETL: A robust digestion layer that seamlessly integrates the different levels of structure.  \\n  \\n3. Feature-Store: An essential layer I\\'d introduce between the digestion and graph database. This acts as a bridge for User Memory Retrieval, Suggestion Storage, and Version Control Implementation. Its primary role?  \\n  \\n--> To intuitively model the problem spaces of team members, ensuring a more personalized and efficient data interaction. <--  \\n  \\n4. Graph-DB: This maps data relationships, providing a comprehensive view of interconnected information. It can also be useful for rebalancing the embeddings-space in context.  \\nThanks to\\xa0John Chong Min Tan, see 34:30 ::\\xa0[https://lnkd.in/eXtwjipz](https://lnkd.in/eXtwjipz)  \\n  \\n5. Service-Mesh (uhhh, the bad-word has been used ;P ): The final layer ensuring smooth communication between Developers & Data Engineers and Scientists & Analysts, as well as machine-to-machine interactions.  \\n  \\nIn the future, one can also consider an\\xa0[#ai](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)\\xa0[#gai](https://www.linkedin.com/feed/hashtag/?keywords=gai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7107439258352640001)\\xa0agent-pool-handler approach to leverage agent behavior among the respective contextually-connected services.  \\n  \\nThanks to\\xa0Ben Lackey\\xa0Architecture Overview:\\xa0[https://lnkd.in/eepxZaKz](https://lnkd.in/eepxZaKz)\\n\\n\\n# 4\\n\\nThat\\'s an interesting take on\\xa0[#llms](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7099107218129571840)\\xa0of Mr. Stephen Wolfram in\\xa0[https://lnkd.in/eQ9bDpBZ](https://lnkd.in/eQ9bDpBZ):  \\nUse\\xa0[#llms](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7099107218129571840)\\xa0models as models of the human brain (that\\'s why we perceive the answers of LLM powered apps like\\xa0[#gpt](https://www.linkedin.com/feed/hashtag/?keywords=gpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7099107218129571840)\\xa0as so astonishing, because they manage to model the way we think and provide the correct extrapolation as the answer to our prompts (sometimes)).  \\nConcretely speaking:  \\nWill it be possible to determine neuroplasticity & synaptic architecture of an individual based on a their interactions with a LLM (IV: Kind of LLM interactions, DV: neuroplasticity & synaptic architecture)?  \\nIf anybody has some intel (papers, references etc.) on that or interpreted the video differently, let me know (I\\'m interested in that kind of research, check out:\\xa0[https://lnkd.in/eCjANtZz](https://lnkd.in/eCjANtZz)).\\n\\n\\n# 5 \\n\\nwhisper api of\\xa0[#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0speech to text is very powerful.  \\nCheck out my app:  \\n[https://lnkd.in/eZpcgKHp](https://lnkd.in/eZpcgKHp)  \\nif you want to try yourself. If you need a quick intro, check out the video.  \\nSorry for getting distracted about\\xa0[#startup](https://www.linkedin.com/feed/hashtag/?keywords=startup&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0ideas :D  \\n&& the translation part comes in v2, for now just multilingual  \\n  \\n--> update: v2 is deployed, same link. Video vor v2:\\xa0[https://lnkd.in/ecZ_a5uG](https://lnkd.in/ecZ_a5uG)  \\n  \\n[#gpt](https://www.linkedin.com/feed/hashtag/?keywords=gpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0[#whisper](https://www.linkedin.com/feed/hashtag/?keywords=whisper&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0[#openaichatgpt](https://www.linkedin.com/feed/hashtag/?keywords=openaichatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0[#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0[#api](https://www.linkedin.com/feed/hashtag/?keywords=api&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\xa0[#ai](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7095892204031762432)\\n\\n\\n# 6 \\n\\nğŸŒ± A Greener Future: Exploring the Impact of Straw Houses on the Carbon Footprint  \\n1. Short Summary:  \\nWith climate change at the forefront of global challenges, innovative solutions are needed more than ever. Building homes with straw could be one such solution, subtracting CO2 from the atmosphere rather than adding to it. This project explores the potential of straw houses to reduce the carbon footprint of the housing industry, considering factors like degradation over time and exponential growth in straw house construction.  \\n2. See Video Below:  \\nWant to understand the concept better? Watch this video from\\xa0[Patagonia](https://www.linkedin.com/company/patagonia_2/)\\xa0Founder Yvon Chouinard, providing insights into the idea of building with straw and its environmental benefits.\\xa0[https://lnkd.in/egyVAKxq](https://lnkd.in/egyVAKxq)  \\n3. See App to Dig Deeper:  \\nReady to explore simulations of such an impact? Dive into my dash app. Analyze global trends or specific countries, see the potential reductions, and even adjust the analysis based on different starting decades (i used 1950 as base scenario, meaning what could have happend if we started back then) ğŸ“Š\\xa0[https://lnkd.in/ei3YYHGT](https://lnkd.in/ei3YYHGT)  \\n4. See Math Explanation:  \\nCurious about the mathematical models behind the analysis? Check out the detailed explanation, including the equations and assumptions used in the calculations. ğŸ§® View the math under the tab \"Math Explanation\" in same app. I used \"Our World in Data\" data:\\xa0[https://github.com/owid](https://github.com/owid)\\xa0. Heads up, straw like ice like all other solutions as of now only store temporarily, so i had to introduce a degradation of the reduction effect of building straw houses.\\xa0  \\n5. See GitHub Repo:  \\nFor fellow developers and data enthusiasts, the entire codebase is available on\\xa0[#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\\xa0:\\xa0[https://lnkd.in/errqTTEr](https://lnkd.in/errqTTEr)  \\nBy embracing innovative construction methods, we can work together to flatten the CO2 curve and build a more sustainable future. Let\\'s explore, analyze, and innovate! ğŸŒğŸ’¡  \\n[#Sustainability](https://www.linkedin.com/feed/hashtag/?keywords=sustainability&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\\xa0[#ClimateChange](https://www.linkedin.com/feed/hashtag/?keywords=climatechange&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\\xa0[#Innovation](https://www.linkedin.com/feed/hashtag/?keywords=innovation&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\\xa0[#StrawHouses](https://www.linkedin.com/feed/hashtag/?keywords=strawhouses&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\\xa0[#DataAnalysis](https://www.linkedin.com/feed/hashtag/?keywords=dataanalysis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7092799964925284352)\\n\\n\\n# 7 \\n\\nIn 2020, I dedicated my BA thesis to delve deep into the realm of data-driven decisions in response to the climate crisis. The introduction to my thesis read:  \\n  \\n\"In response to the climate debate, data-driven decisions are increasingly relied upon in Europe. These technocratic decisions (as they are often called in a political context)(cf. Radaelli, 2017) are behind national sanctions like the ones in force in the German city Stuttgart for exceeding the threshold value for air pollution. Because of the all-englobing nature of the problem of climate change, nearly every sector (primary, secondary and tertiary sector) of the economy is touched. But do all stakeholders in these sectors have access to reliable data? Does any citizen, policy maker, or scientist in Europe have access to consistent and reliable data? Does the mayor of a small village in Germany have access to the same environmental data as a policy maker in the EU or any other mayor in the EU. These questions are addressed in the work at hand for the member states France and Germany. The accessibility of data is critically necessary for the social acceptance of the legislation of increasingly stringent environmental measures (cf. Carrete et al., 2012). This principle dovetails with one of the main principles of the Balanced-Scorecard-Topic: â€ If you canâ€™t measure it, you canâ€™t manage it. â€(Kaplan and David, 2000, p. 21) or in this context: you canâ€™t make effective managerial decisions without reliable access to consistent data.â€  \\n  \\nYou can find the complete thesis here:\\xa0[https://lnkd.in/eNMHySsr](https://lnkd.in/eNMHySsr)  \\n  \\nI\\'m gratified to see that innovative start-ups like Regionalwert Research:\\xa0[https://lnkd.in/ek4eikJF](https://lnkd.in/ek4eikJF)\\xa0and\\xa0the promising @Isometric\\xa0[https://lnkd.in/es_Raq82](https://lnkd.in/es_Raq82)\\xa0are harnessing the power of data to make informed decisions. These organizations are paving the way to a future where data accessibility isn\\'t a privilege but a norm.  \\n  \\nLet\\'s empower everyone, from policy-makers to citizens, with the data they need to make informed decisions about our planet\\'s future.  \\n  \\n[#DataDrivenDecisions](https://www.linkedin.com/feed/hashtag/?keywords=datadrivendecisions&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7087193892416012288)\\xa0[#ClimateCrisis](https://www.linkedin.com/feed/hashtag/?keywords=climatecrisis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7087193892416012288)\\xa0[#DataAccessibility](https://www.linkedin.com/feed/hashtag/?keywords=dataaccessibility&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7087193892416012288)\\n\\n\\n# 8 \\n\\nğŸš€Excited to share simple Approach for Unraveling the mystery of JSON extraction in BQ!ğŸ”  \\n  \\nâ¡ï¸Coming up next: keep an eye out for our mini-series on Parameterized Queries, and getting hands-on with Dataform and Apache Beam!  \\n  \\n[#BigQuery](https://www.linkedin.com/feed/hashtag/?keywords=bigquery&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\xa0[#GoogleCloud](https://www.linkedin.com/feed/hashtag/?keywords=googlecloud&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\xa0[#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\xa0[#Dataform](https://www.linkedin.com/feed/hashtag/?keywords=dataform&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\xa0[#ApacheBeam](https://www.linkedin.com/feed/hashtag/?keywords=apachebeam&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\xa0[#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\xa0[#TutorialSeries](https://www.linkedin.com/feed/hashtag/?keywords=tutorialseries&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7086494437509357568)\\n\\n\\n# 9 \\n\\nDelighted to share a resource that could potentially help all those who still shudder at the mention of high school math. Here\\'s a video that might just be the remedy you need:  \\nğŸ¥ Animated Math Tutorial - Kudos to @alanbecker for making math more approachable and fun!\\xa0[https://lnkd.in/ext6wHDN](https://lnkd.in/ext6wHDN)  \\nAnd for those of you who are looking to dive deeper into the analogies and explore a deeper understanding, I highly recommend watching the following \\'lyrics\\' video:  \\nğŸ¥ Math Analogy Explained - Big shoutout to @prosperpython for their insightful commentary!\\xa0[https://lnkd.in/epEyePxE](https://lnkd.in/epEyePxE)  \\nTake a moment out of your busy schedules to appreciate the beauty of math explained in an engaging and enjoyable manner. Happy learning! ğŸ“š  \\n[#MathMadeEasy](https://www.linkedin.com/feed/hashtag/?keywords=mathmadeeasy&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)\\xa0[#LearningResources](https://www.linkedin.com/feed/hashtag/?keywords=learningresources&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)\\xa0[#AlanBecker](https://www.linkedin.com/feed/hashtag/?keywords=alanbecker&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)\\xa0[#ProsperPython](https://www.linkedin.com/feed/hashtag/?keywords=prosperpython&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)\\xa0[#Mathematics](https://www.linkedin.com/feed/hashtag/?keywords=mathematics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7083905605932867584)\\n\\n\\n# 10 \\n\\nI beg your indulgence as I once again delve into the delightful realm of analogy-laden musings (honestly, i have the best time and i guess you guys have to suffer :D ):  \\n  \\nğŸ¬ Like the characters in \\'Inception\\' and \\'Predestination\\', I navigate through layers of reality - only, my layers are not dreams or timelines, but data-sets and tables in DBs. In our latest project, I\\'ve designed a class, which weaves through these data-scapes, building views, fetching the latest tables, and generating summaries for reports.  \\n  \\nOur raw data reports remain static, a frozen snapshot of a moment in time. Just as in quantum theory, where a particle\\'s state isn\\'t determined until it\\'s observed (principle of superposition), our raw data only unfolds its patterns and trends when the view is invoked.  \\n  \\nJust as a clockmaker peers into the intricate labyrinths of cogs and springs, refining their dance within the heart of a timepiece, I too navigate the maze of data in the realm of temporal dimensions.  \\n  \\nOr just as the agents in \\'[#inception](https://www.linkedin.com/feed/hashtag/?keywords=inception&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\' create their dream world, my class generates temporary tables from these views, crafting a unique vision of the data landscape.  \\nIt\\'s like navigating through a world that is constantly being rewritten, much like how the time agent\\'s actions constantly reshaped his timeline in \\'Predestination\\'(Prudhvee Krishna Nalluri, thx. mate!ğŸ‘ŒğŸ¥).  \\nHe voyages through the weft and weave of time to thwart calamities before they occur. Similar to him, I delve into the intricacies of the most recent data tables to identify and mitigate any potential issues before they occur.  \\n  \\nIn the end, we form a dance between creation and destruction, viewing and refreshing, much like the ebb and flow of time in our sci-fi thrillers. It\\'s an intricate game that transforms raw, unobserved data into rich, meaningful insights, ready to drive decision-making in this ever-evolving data-driven world.  \\n  \\n[#BigQuery](https://www.linkedin.com/feed/hashtag/?keywords=bigquery&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\xa0[#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\xa0[#Python](https://www.linkedin.com/feed/hashtag/?keywords=python&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\xa0[#SciFiMetaphors](https://www.linkedin.com/feed/hashtag/?keywords=scifimetaphors&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\xa0[#Inception](https://www.linkedin.com/feed/hashtag/?keywords=inception&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\xa0[#Predestination](https://www.linkedin.com/feed/hashtag/?keywords=predestination&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7077670585358528512)\\n\\n\\n# 11\\n\\nHello Weekend-Hustlers! ğŸŒ  \\nStraight from the heart of the Four Corners of Civilization, I\\'m going to share an opinionated process-approach that\\'s been my Fae companion in the treacherous journey through the coding Edema Ruh troupe.  \\nUse-Case: Think of projects that resemble the intricate harmony between the Arcanum and the University, where the silent communication between an Artificer and their Kilvin is crucial, and our Sympathy links with monitoring and testing are our saving grace.  \\nConsider the following as a pseudo-code class: TheKingKillerChronicle. This is an epic saga, not unlike Kvothe\\'s own tale, where each class and function is a character. The story revolves around dancing with the ever-lurking Cthaeh, AKA Conway\\'s Law, without being bitten.  \\nClass TheKingKillerChronicle inherits from RuhLore:  \\nğŸ”¹ Method 1 - Design Workshop: Like young Kvothe learning the Edema Ruh\\'s ways, our Sympathist class acquires domain-specific terms from the stakeholder. Learning outcomes, SLAs, and value errors are etched in the Waystone Inn\\'s ledger, in our case, UML. This is where our tale begins, a common tongue is agreed upon, and we lay the groundwork for our defense against the Cthaeh\\'s prophecies.  \\nğŸ”¹ Method 2 - Infrastructure as Code (IaC): Picture this as Elodin\\'s Naming class, granting first-class citizenship to your infrastructure in the realm of code. Whether it\\'s the primal essence of Terraform or the syntactic consistency of CloudFormation, your infrastructure will hum along like the wind called by a namer.  \\nğŸ”¹ Method 3 - Meta-Decorator: This is like a more versatile version of Mola\\'s careful administrations in the Medica - a log_and_test() function on a potent alchemical mixture. From translating SLAs into tangible elements and monitoring them like the Masters watch the Crockery, to configuring errors and redundancies, this method is as meticulous as a well-performed Binding.  \\nğŸ”¹ Method 4 - Architecture Feedback Loop: The grand finale, the closing notes of Kvothe\\'s lute at the Eolian. We convene with stakeholders, our resolution map in hand, and hold it up to the golden standard set in the Design Workshop. With print(help(ClassOne)) as our lens, we refine it until it becomes a song that will echo through the centuries.  \\nCheck out the attached UML diagram for a glimpse into our saga, in a form even the Archives couldn\\'t misplace.  \\nIn the end, the most skilled Shapers aren\\'t just those fluent in the arcane languages of code, but also those adept in the arts of communication, planning, and feedback. As they say in the Commonwealth, may all your stories be glad ones! ğŸ»  \\n[#CodingLife](https://www.linkedin.com/feed/hashtag/?keywords=codinglife&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)\\xa0[#SoftwareDesign](https://www.linkedin.com/feed/hashtag/?keywords=softwaredesign&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)\\xa0[#WeekendHustle](https://www.linkedin.com/feed/hashtag/?keywords=weekendhustle&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)\\xa0[#DevOps](https://www.linkedin.com/feed/hashtag/?keywords=devops&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)\\xa0[#ContinuousLearning](https://www.linkedin.com/feed/hashtag/?keywords=continuouslearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7075939490128637952)\\n\\n\\n\\n# 12\\n\\n[#attentionIsAllYouNeed](https://www.linkedin.com/feed/hashtag/?keywords=attentionisallyouneed&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7071455186656874496)\\xa0thanks\\xa0[Joshua Starmer PhD](https://www.linkedin.com/in/ACoAACAd8VYBqvN-leU7VPrO93tKYbEfzhAFOgY)  \\n[https://lnkd.in/evk3t5G3](https://lnkd.in/evk3t5G3)  \\n[#triplebam](https://www.linkedin.com/feed/hashtag/?keywords=triplebam&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7071455186656874496)  \\n[https://lnkd.in/erFSNUym](https://lnkd.in/erFSNUym)\\n\\n\\n\\n# 13\\n\\nOf many nice frameworks for\\xa0[#graph](https://www.linkedin.com/feed/hashtag/?keywords=graph&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177)\\xa0[#visualization](https://www.linkedin.com/feed/hashtag/?keywords=visualization&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177), there is one which is for me at the absolute top:\\xa0[https://threejs.org/](https://threejs.org/)\\xa0and one dev who pushes its boundaries: Andrei Kashcha (big-up!), i can only hope that i reach one day his level:  \\n[https://lnkd.in/evRgE_Tv](https://lnkd.in/evRgE_Tv)  \\n(please, don\\'t forget to zoom in)  \\nAlso nice to use:  \\n1.\\xa0[https://d3js.org/](https://d3js.org/)  \\n2.\\xa0[https://viz-js.com/](https://viz-js.com/)  \\n3.\\xa0[https://lnkd.in/eFMGJRHq](https://lnkd.in/eFMGJRHq)  \\n  \\nAnd here for my academic acquaintances:  \\n[https://lnkd.in/eimR3Und](https://lnkd.in/eimR3Und)  \\n  \\nAnd for those who ever wanted to see\\xa0[#rstats](https://www.linkedin.com/feed/hashtag/?keywords=rstats&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177)\\xa0and\\xa0[#python](https://www.linkedin.com/feed/hashtag/?keywords=python&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7069024910508978177)\\xa0packages and their dependencies visualized:  \\n[https://lnkd.in/ei-yJhPB](https://lnkd.in/ei-yJhPB)\\n\\n\\n# 14\\n\\nFor anyone who didnt hear the news,\\xa0[#vertexai](https://www.linkedin.com/feed/hashtag/?keywords=vertexai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7062178683528060928)\\xa0drawn equal with\\xa0[#sagemaker](https://www.linkedin.com/feed/hashtag/?keywords=sagemaker&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7062178683528060928)\\xa0regarding\\xa0[#llms](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7062178683528060928)\\xa0[functionality-offer wise]\\n\\n\\n# 15 \\n\\nMind following aspects when starting training & fine tuning\\xa0[#LLMs](https://www.linkedin.com/feed/hashtag/?keywords=llms&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008):  \\n1. Don\\'t waste time and money and try and run a\\xa0[#LLM](https://www.linkedin.com/feed/hashtag/?keywords=llm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008)\\xa0on a notebook, you will get cuda out of memory errors all the time, because the cuda driver doesnt know how to handle multiple notebooks or even if you open a console etc. it will have problems. And dont get me started on server disconnects :D  \\n2. If you go for an instance, go for the g-family in\\xa0[#ec2](https://www.linkedin.com/feed/hashtag/?keywords=ec2&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008), the p-family gave me problems with the\\xa0[#cuda](https://www.linkedin.com/feed/hashtag/?keywords=cuda&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7051614286724395008)\\xa0driver  \\n3. Start to understand the connection between mirco-batches, RAM and learning-rate, if not your convergance gonna mess up your results  \\n4. Start to parallelize your LLM training and fine tuning, espacially if you run one more than one gpu, if not your money will be waisted on the other gpus.\\n\\n\\n# 16 \\n\\n\\nI fine tuned like a dozen\\xa0[#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7046189214668836864)\\xa0models (ada, davinci, curie etc.), in all of them i got:  \\n  \\nJob complete! Status: succeeded ğŸ‰  \\n... but one:  \\nJob complete! Status: succeeded Ã°ï¿½ï¿½ï¿½  \\n  \\nAnyone any idea what Eth ( Ã°) is doing there? :D  \\n  \\nOne further note here:  \\nYou can further fine tune your already fine tuned models. So you can basically fine tune model a for agency a and model b for agency b and then you make model c based on agency model a and b and so you can build your company-knowledge-[#ai](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7046189214668836864)\\xa0in a layered logic.  \\n  \\nAnd here a small batch script so you\\'re not stuck in a queue while fine tuning:D  \\n  \\n#!/usr/bin/expect -f  \\n  \\n# set the timeout to 1 second  \\nset timeout 1  \\n  \\n# define the command to be run  \\nset COMMAND \"openai api fine_tunes.follow -i ft-XXX\"  \\n  \\n# spawn a new shell and start the command  \\nspawn sh  \\n  \\nsend \"$COMMAND\\\\n\"  \\n  \\n# continuously monitor the console output for \"disconnect\"  \\nwhile true {  \\nexpect {  \\n-re \".*disconnect.*\" {  \\n# if \"disconnect\" is found, run the command again  \\nputs \"Connection lost! Reconnecting...\"  \\nsend \"$COMMAND\\\\n\"  \\n}  \\ntimeout {  \\n# wait for 1 second before checking again  \\nsleep 1  \\n}  \\n}  \\n}\\n\\n\\n# 18 \\n\\nHappy birthday\\xa0[#patagonia](https://www.linkedin.com/feed/hashtag/?keywords=patagonia&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7042849455431139328)\\xa0[#responsibility](https://www.linkedin.com/feed/hashtag/?keywords=responsibility&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7042849455431139328)\\n\\n\\n# 19 \\n\\nFor all of you who had to do hard string operations with high server costs in the past. These days are over, thanks to\\xa0[#openai](https://www.linkedin.com/feed/hashtag/?keywords=openai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040370029950873601)  \\nGo for Fine-Tuning:\\xa0[https://lnkd.in/dK2jWTgr](https://lnkd.in/dK2jWTgr)  \\n  \\nYour models will appear afterwards in the playground and you can use them in your apis.  \\n  \\nPitfalls at the moment:  \\n1. Could solve nor find solution for following error:  \\n\\'The number of classes in file does not match the number of classes specified in the hyperparameters.\\'  \\n2. You will have a lot of server disconnect messages and you will be stuck in a queu for a few minutes\\n\\n\\n\\n# 20 \\n\\nHi, I have developed an app based on\\xa0[REWE](https://www.linkedin.com/company/rewe/)\\xa0[REWE Group](https://www.linkedin.com/company/rewegroup/)\\xa0E-Bon, because unfortunately you can only get it as a pdf via the rewe app and only aggergated by purchase trip as csv, but I wanted to have the listing of products via csv. You can use the app to convert the receipt into a csv and if you want to do me something good, you can also provide it to me. Mind that it\\'s still in Beta. More here:  \\n  \\n[https://lnkd.in/e2fBFmsR](https://lnkd.in/e2fBFmsR)\\n\\n\\n\\n# 21\\n\\nThe beauty of layering APIs ([#chatgpt](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)\\xa0[#wolfram](https://www.linkedin.com/feed/hashtag/?keywords=wolfram&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)\\xa0[#whisper](https://www.linkedin.com/feed/hashtag/?keywords=whisper&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)) each one playing on it\\'s strength &\\xa0[#huggingface](https://www.linkedin.com/feed/hashtag/?keywords=huggingface&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)\\xa0has direct integration to\\xa0[#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)\\xa0[#sagemaker](https://www.linkedin.com/feed/hashtag/?keywords=sagemaker&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)  \\nGame is on for\\xa0[#AI_Composing](https://www.linkedin.com/feed/hashtag/?keywords=ai_composing&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7035742155650277376)\\n\\n\\n# 22\\nWant to get started with applied reinforcement learning?  \\nFirst get gym.  \\nFollowing tutorial will help you to set it up correctly:  \\n[https://lnkd.in/emyPPUaf](https://lnkd.in/emyPPUaf)  \\n[Dibya Chakravorty](https://www.linkedin.com/in/ACoAABmJ5cgBCVP-7JVVogFK7flj2iVNyuedHP4)\\xa0thx. mate :)\\n\\n\\n# 23\\n\\nNew use-case for\\xa0[#chatgpt](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)\\xa0you can map all kinds of\\xa0[#networks](https://www.linkedin.com/feed/hashtag/?keywords=networks&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)\\xa0from your cdrive to your company\\xa0[#sharepoint](https://www.linkedin.com/feed/hashtag/?keywords=sharepoint&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344), use this repo:\\xa0[https://lnkd.in/gT2cznKj](https://lnkd.in/gT2cznKj)\\xa0and if you already have\\xa0[#neo4j](https://www.linkedin.com/feed/hashtag/?keywords=neo4j&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)\\xa0, you can compare your indexing against the one\\xa0[#chatgpt](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7026661448399417344)\\xa0would apply :D\\n\\n\\n# 24\\n\\n[#gcp](https://www.linkedin.com/feed/hashtag/?keywords=gcp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7025949249414590465)\\xa0[#bigquery](https://www.linkedin.com/feed/hashtag/?keywords=bigquery&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7025949249414590465)\\xa0has now the feature lineage and execution graph: In case you wonder how to improve your query and make it cost efficient\\n\\n\\n# Just read an article that could provide a real-time feedback-loop not only for logistic-models but also for retailer-models, imagine, how much faster the iterations would run through ::\\xa0[https://lnkd.in/g9_vbF3F](https://lnkd.in/g9_vbF3F)\\xa0[#rewe](https://www.linkedin.com/feed/hashtag/?keywords=rewe&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([REWE](https://www.linkedin.com/company/rewe/),\\xa0[REWE Group](https://www.linkedin.com/company/rewegroup/)),\\xa0[#lidl](https://www.linkedin.com/feed/hashtag/?keywords=lidl&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([Lidl in Germany](https://www.linkedin.com/company/lidl-in-deutschland/),\\xa0[Lidl Portugal](https://www.linkedin.com/company/lidlportugal/)),\\xa0[#aldi](https://www.linkedin.com/feed/hashtag/?keywords=aldi&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([ALDI SÃœD](https://www.linkedin.com/company/aldi-sued/),\\xa0[ALDI USA](https://www.linkedin.com/company/aldi-usa/)),\\xa0[#penny](https://www.linkedin.com/feed/hashtag/?keywords=penny&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072),\\xa0[#netto](https://www.linkedin.com/feed/hashtag/?keywords=netto&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([Netto Marken-Discount](https://www.linkedin.com/company/netto-marken-discount/),\\xa0[Netto](https://www.linkedin.com/company/netto/)),\\xa0[#kaufland](https://www.linkedin.com/feed/hashtag/?keywords=kaufland&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([Kaufland](https://www.linkedin.com/company/kaufland-deutschland/),\\xa0[Kaufland e-commerce](https://www.linkedin.com/company/kauflandecommerce/)),\\xa0[#real](https://www.linkedin.com/feed/hashtag/?keywords=real&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0,\\xa0[#edeka](https://www.linkedin.com/feed/hashtag/?keywords=edeka&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([EDEKA](https://www.linkedin.com/company/edeka/)),\\xa0[#dm](https://www.linkedin.com/feed/hashtag/?keywords=dm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072),\\xa0[#denns](https://www.linkedin.com/feed/hashtag/?keywords=denns&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([Denns BioMarkt](https://www.linkedin.com/company/denns-biomarkt-gmbh/)\\xa0),\\xa0[#alnatura](https://www.linkedin.com/feed/hashtag/?keywords=alnatura&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072)\\xa0([Alnatura](https://www.linkedin.com/company/alnatura/))\\n\\n\\n\\n# 25 \\n\\nFor all those out there who ever wondered what\\xa0[#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6999448747759783937)\\xa0is, here is a very fresh intro of\\xa0[#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6999448747759783937)\\xa0itself.  \\n[#github](https://www.linkedin.com/feed/hashtag/?keywords=github&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6999448747759783937)\\xa0[GitHub](https://www.linkedin.com/company/github/)\\xa0guys, thanks for everything.\\n\\n# 26 \\n\\nmaybe interesting for anyone who does\\xa0[#onlinebanking](https://www.linkedin.com/feed/hashtag/?keywords=onlinebanking&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6996584105404420096)\\xa0:  \\nI belive, that a product is as good as its\\xa0[#community](https://www.linkedin.com/feed/hashtag/?keywords=community&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6996584105404420096)\\xa0(e.g. look at\\xa0[#battelfield](https://www.linkedin.com/feed/hashtag/?keywords=battelfield&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6996584105404420096)\\xa02042 ;D).  \\nThus, as a customer of\\xa0[comdirect â€“ eine Marke der Commerzbank AG](https://www.linkedin.com/company/comdirect-bank/)\\xa0I\\'m grateful, that you have an api (see\\xa0[https://lnkd.in/e7jp4WwV](https://lnkd.in/e7jp4WwV)).  \\n  \\nBut would you pls. make a read-only / reporting access with the chance to auto auth. instead of just giving the chance to make a read & WRITE access to a private online account? :D  \\nAs a banchmark you can take\\xa0[Wise](https://www.linkedin.com/company/wiseaccount/), they did it right ;P  \\n  \\n--> Updt.: Found a nice workaround: use\\xa0[https://lnkd.in/eqsVzZrR](https://lnkd.in/eqsVzZrR)\\xa0, they use the api nicely and we\\'re out of the risky \"own dev\" scope.\\n\\n\\n# 27 \\n\\nHere\\'s a\\xa0[#startup](https://www.linkedin.com/feed/hashtag/?keywords=startup&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040)\\xa0idea: using AI,\\xa0[#DALLÂ·E](https://www.linkedin.com/feed/hashtag/?keywords=dall%C2%B7e&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040)\\xa0or rather\\xa0[#DALLÂ·Es](https://www.linkedin.com/feed/hashtag/?keywords=dall%C2%B7es&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040)\\xa0interpretation (in stats terms: the noise, playing around with the schedule) as mediator between sides (whatever they may be). Here\\'s a beautiful example that kept me awake the whole night:  \\n  \\nI asked\\xa0[#DALLÂ·E](https://www.linkedin.com/feed/hashtag/?keywords=dall%C2%B7e&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6989759032395735040):  \\n\"the discours between the Standard Model of Particle Physics with the subatomic particle physics interpreted out of the Langlands program perspective of uniting\\xa0number theory and geometry represented in a graphical color mix\"  \\n  \\nAnybody up for it? ;P\\n\\n\\n# 28 \\n\\nHi guys,  \\njust a quick heads up of for those interested in\\xa0[#spark](https://www.linkedin.com/feed/hashtag/?keywords=spark&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)\\xa0[#r](https://www.linkedin.com/feed/hashtag/?keywords=r&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)\\xa0[#python](https://www.linkedin.com/feed/hashtag/?keywords=python&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)\\xa0:: The\\xa0[#apachespark](https://www.linkedin.com/feed/hashtag/?keywords=apachespark&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)\\xa0team slowly extends the documentation for\\xa0[#r](https://www.linkedin.com/feed/hashtag/?keywords=r&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6978785041724121090)\\xa0(e.g. One-vs-All missing in .r). Great to see that the bilinguality in the statistical world evolves ; * ::\\n\\n\\n# 29 \\n\\nIn the off chance, that you work with x>2 TB in pyspark (look below), if not, stop reading HERE, be happy and enjoy the summer :)  \\n\\xa0  \\n1. Use zeppelin notebook instead of jupyter (if you don\\'t believe me, try to cache in a jupyter nb. More then 2 Tb, if you manage, write me)  \\n  \\n2. Use highmem instances if you work on gcp (mind your budget)  \\n  \\n3. Set memory config to x>64g (if you have enough worker nodes), if not, you get a buffer overflow (especially if you do string ops.) [thx. to exodus-intelligence to put me on the topic]  \\n  \\n4. Cache() the data you want to work with (cache and sample, then checkpoint by saving to bucket, then read in and cache again in case you want to make some fancy visualization)  \\n  \\n5. Before you start with any modelling at all, try to reduce the dimensions (your columns/ variables) as far as possible (use pairplot to get a first idea, then go for pca, plot dimensions as overlay to datapoints)  \\n\\xa0  \\nThx. To FN, SG, InS for those insights.  \\n\\xa0  \\nP.S. If there is anybody out there, who managed to run sparklyr on a zeppelin notebook, pls. Write me, I will buy you a bottle of wine (or juice).\\n\\n\\n# 30 \\n\\nKnowledge is sth. beautiful, but as long as we don\\'t structure it, it\\'s ambiguous, and misunderstandins are prone to happen.  \\n  \\nI wrote a function in .r (kind of a parser), which translates my former knowledge-DB (.xml) to obsidian (auto generate the .md). Check out the beauty of it ;P  \\n  \\nFor the future I want to integrate\\xa0[#rstudio](https://www.linkedin.com/feed/hashtag/?keywords=rstudio&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)\\xa0,\\xa0[#wiki](https://www.linkedin.com/feed/hashtag/?keywords=wiki&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)\\xa0.js &\\xa0[#obsidian](https://www.linkedin.com/feed/hashtag/?keywords=obsidian&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)\\xa0(and as a long-shot the\\xa0[#termstore](https://www.linkedin.com/feed/hashtag/?keywords=termstore&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)\\xa0of\\xa0[#sharepoint](https://www.linkedin.com/feed/hashtag/?keywords=sharepoint&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6944025287063969792)\\xa0), to bring the best of all worlds together and stcuture my scripts & data & meta-data in a new way. Somebody any experience with that integration?\\n\\n\\n# 31\\n\\nFor those who ever wondered why the\\xa0[#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6899827928369127424)\\xa0documentation:\\xa0[https://lnkd.in/gKS6K3Qb](https://lnkd.in/gKS6K3Qb)\\xa0regarding setting up an EMR R-Studio spark cluster doesn\\'t work for you, don\\'t worry, it\\'s not your fault.\\xa0[#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6899827928369127424)\\xa0doesn\\'t care to keep the documentation updated! I\\'m happy to supply you with my build file:  \\n[https://lnkd.in/gjR3iscZ](https://lnkd.in/gjR3iscZ)\\xa0(Remember to change default PW!)  \\nOne other thing,\\xa0[#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6899827928369127424)\\xa0also doesn\\'t allocate all worker nodes to spark, you need to use:  \\n[ { \"Classification\": \"spark\", \"Properties\": { \"maximizeResourceAllocation\": \"true\" } } ]\\n\\n\\n# 32\\n\\nFor all of those who love\\xa0[#rshiny](https://www.linkedin.com/feed/hashtag/?keywords=rshiny&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0and want to asses their dev-approach or just want to get rid of the spaghetti bowl (Eric Nantz - Ian Lyttle) here comes a smooth approach: Use reactlog\\xa0[https://lnkd.in/guBCi79a](https://lnkd.in/guBCi79a)  \\n  \\nWhile doing one of my rare posts:  \\nThis post helps to get in to\\xa0[#nlp](https://www.linkedin.com/feed/hashtag/?keywords=nlp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0[https://lnkd.in/guaiJH8v](https://lnkd.in/guaiJH8v)  \\n\\xa0  \\nBut for those searching for the right library to vis\\xa0[#nlp](https://www.linkedin.com/feed/hashtag/?keywords=nlp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0[#visualization](https://www.linkedin.com/feed/hashtag/?keywords=visualization&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0[#interactivly](https://www.linkedin.com/feed/hashtag/?keywords=interactivly&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112), cheers to \"networkD3\"\\xa0[https://lnkd.in/grjVXbqM](https://lnkd.in/grjVXbqM)\\xa0- great job guys!  \\n\\xa0  \\nUse Case:\\xa0thx. To\\xa0[#PetitBiscuit](https://www.linkedin.com/feed/hashtag/?keywords=petitbiscuit&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0- We Were Young ft. JP Cooper (Lyrics)\\xa0\\xa0[https://lnkd.in/gX6ndzeR](https://lnkd.in/gX6ndzeR)\\xa0for making good music and\\xa0[#youtube](https://www.linkedin.com/feed/hashtag/?keywords=youtube&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0for providing transcripts (I have to benchmark it next time against\\xa0[#aws](https://www.linkedin.com/feed/hashtag/?keywords=aws&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\xa0transcriber and\\xa0[#gcp](https://www.linkedin.com/feed/hashtag/?keywords=gcp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6897287324867162112)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb247e00-752d-43e5-bad6-055b69473da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(You, decided to share, conversation countries)\n",
      "(You, decided to share, global open-source network)\n",
      "(You, would be contributing, collective knowledge pool)\n",
      "(You, would be exposing, myriad of question-answer universes)\n",
      "(Gandhi, Be the change you want to see in the world)\n",
      "(I, have honestly no idea, happens if more than one person decides to continue the conversation)\n",
      "(Please feel free to be part, social experiment)\n",
      "(Answer and share again, ripple effect)\n",
      "(companies, have more than 1000 employees)\n",
      "(companies, need a transformative approach)\n",
      "(data lake, is, bedrock of data storage and management)\n",
      "(ELT, is, robust digestion layer)\n",
      "(feature-store, is, bridge for User Memory Retrieval)\n",
      "(graph-DB, is, maps data relationships)\n",
      "(Service-Mesh, ensures communication between, Developers & Data Engineers)\n",
      "(Service-Mesh, ensures communication between, Scientists & Analysts)\n",
      "(Service-Mesh, ensures communication between, machines)\n",
      "(Service-Mesh, can consider, agent-pool-handler approach)\n",
      "(Service-Mesh, can consider, agent behavior among services)\n",
      "(whisper api, is, speech to text)\n",
      "(whisper api, of, openai)\n",
      "(whisper api, is, very powerful)\n",
      "(app, is, [https://lnkd.in/eZpcgKHp](https://lnkd.in/eZpcgKHp))\n",
      "(app, check out video for, intro)\n",
      "(app, is, v2)\n",
      "(app, is, multilingual)\n",
      "(Straw houses, could reduce, carbon footprint)\n",
      "(Straw houses, subtracting, CO2)\n",
      "(Straw houses, considering factors like, degradation over time)\n",
      "(Straw houses, exponential growth in, straw house construction)\n",
      "(straw, has environmental benefits, )\n",
      "(straw, can be used for building, )\n",
      "(building with straw, can reduce CO2 emissions, )\n",
      "(data-driven decisions, are increasingly relied upon, in Europe)\n",
      "(data-driven decisions, are often called, technocratic decisions)\n",
      "(data-driven decisions, are behind, national sanctions)\n",
      "(climate change, touches, nearly every sector)\n",
      "(stakeholders, have access to, reliable data)\n",
      "(citizens, have access to, consistent and reliable data)\n",
      "(work, addresses, environmental measures)\n",
      "(data, is necessary for, social acceptance)\n",
      "(data, is necessary for, effective managerial decisions)\n",
      "(Innovative start-ups, are harnessing, the power of data)\n",
      "(These organizations, are paving the way, to a future)\n",
      "(Let's empower, everyone with, the data they need)\n",
      "(JSON, extraction, BQ)\n",
      "(Parameterized Queries, mini-series, Dataform)\n",
      "(Apache Beam, hands-on, Dataform)\n",
      "(math, is, remedy)\n",
      "(math, is, beautiful)\n",
      "(math, is, enjoyable)\n",
      "(video, is, remedy)\n",
      "(video, is, enjoyable)\n",
      "(video, is, math)\n",
      "(@alanbecker, makes, math)\n",
      "(@prosperpython, makes, math)\n",
      "(@prosperpython, makes, math)\n",
      "(Inception, is, movie)\n",
      "(Predestination, is, movie)\n",
      "(Inception, is about, dreams)\n",
      "(Predestination, is about, time travel)\n",
      "(Inception, is about, reality)\n",
      "(Inception, is about, layers of reality)\n",
      "(Inception, is about, data-sets)\n",
      "(Inception, is about, tables)\n",
      "(Inception, is about, DBs)\n",
      "(Inception, is about, views)\n",
      "(coding, is, Edema Ruh troupe)\n",
      "(coding, is, Four Corners of Civilization)\n",
      "(coding, is, Arcanum and the University)\n",
      "(coding, is, Artificer and their Kilvin)\n",
      "(coding, is, Sympathy links with monitoring and testing)\n",
      "(TheKingKillerChronicle, inherits from, RuhLore)\n",
      "(TheKingKillerChronicle, is, class)\n",
      "(TheKingKillerChronicle, is, epic saga)\n",
      "(TheKingKillerChronicle, is, like Kvothe's own tale)\n",
      "(TheKingKillerChronicle, has, character)\n",
      "(TheKingKillerChronicle, revolves around, dancing with Cthaeh)\n",
      "(TheKingKillerChronicle, has, method 1)\n",
      "(TheKingKillerChronicle, has, method 2)\n",
      "(TheKingKillerChronicle, has, method 3)\n",
      "(TheKingKillerChronicle, has, method 4)\n",
      "(Shapers, skilled in, communication)\n",
      "(Shapers, skilled in, feedback)\n",
      "(Shapers, skilled in, planning)\n",
      "(Shapers, skilled in, coding)\n",
      "(Shapers, say, may all your stories be glad ones)\n",
      "(graph, visualization, threejs.org)\n",
      "(graph, visualization, d3js.org)\n",
      "(VertexAI, is, equal with Sagemaker)\n",
      "(VertexAI, functionality-offer wise, equal with Sagemaker)\n",
      "(LLMS, functionality-offer wise, equal with VertexAI and Sagemaker)\n",
      "(training, mind following aspects, cuda out of memory errors)\n",
      "(training, go for g-family in ec2, p-family gave me problems with cuda driver)\n",
      "(training, understand connection between micro-batches, RAM and learning-rate, convergance will mess up results)\n",
      "(training, parallelize your LLM training and fine tuning, espacially if you run one more than one gpu)\n",
      "(openai, fine tunes, models)\n",
      "(openai, fine tunes, ada)\n",
      "(openai, fine tunes, davinci)\n",
      "(openai, fine tunes, curie)\n",
      "(openai, fine tunes, agency a)\n",
      "(openai, fine tunes, agency b)\n",
      "(openai, fine tunes, company-knowledge-[#ai])\n",
      "(Fine-Tuning, is, Go for)\n",
      "(Pitfalls, are, a lot of server disconnect messages)\n",
      "(REWE, is, company)\n",
      "(REWE Group, is, company)\n",
      "(E-Bon, is, product)\n",
      "(E-Bon, is, receipt)\n",
      "(E-Bon, is, csv)\n",
      "(layering APIs, playing on, strength)\n",
      "(huggingface, has direct integration to, aws sagemaker)\n",
      "(AI composing, game is on for, composing)\n",
      "(chatgpt, can map, networks)\n",
      "(chatgpt, can map, cdrive)\n",
      "(chatgpt, can map, company)\n",
      "(chatgpt, can map, sharepoint)\n",
      "(neo4j, can compare, indexing)\n",
      "(neo4j, can compare, chatgpt)\n",
      "(bigquery, has now the feature, lineage and execution graph)\n",
      "(bigquery, has now the feature, cost efficient)\n",
      "(article, could provide, feedback-loop)\n",
      "(article, could provide, iterations)\n",
      "(retailer-models, imagine, how much faster the iterations would run through)\n",
      "(Aldi, is, company)\n",
      "(Aldi, sued, [ALDI USA](https://www.linkedin.com/company/aldi-usa/))\n",
      "(Aldi, sued, [#penny](https://www.linkedin.com/feed/hashtag/?keywords=penny&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072))\n",
      "(Aldi, sued, [#netto](https://www.linkedin.com/feed/hashtag/?keywords=netto&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072))\n",
      "(Aldi, sued, [#kaufland](https://www.linkedin.com/feed/hashtag/?keywords=kaufland&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7020127850972291072))\n",
      "(Aldi, sued, [#real](https://www.linkedin.com/feed/hashtag/?keywords=real&highlighted\n",
      "(GitHub, is, a web-based hosting service)\n",
      "(GitHub, was founded in, 2008)\n",
      "(GitHub, is headquartered in, San Francisco)\n",
      "(I, believe, that a product is as good as its community)\n",
      "(comdirect, have, an api)\n",
      "(Wise, use, the api nicely)\n",
      "(DALLÂ·E, is, AI)\n",
      "(DALLÂ·E, is, mediator)\n",
      "(DALLÂ·E, is, noise)\n",
      "(DALLÂ·E, is, stats terms)\n",
      "(DALLÂ·E, is, interpretation)\n",
      "(DALLÂ·E, is, uniting number theory)\n",
      "(DALLÂ·E, is, represented in a graphical color mix)\n",
      "(Spark, is, Apache Spark)\n",
      "(Spark, documentation for, R)\n",
      "(Spark, bilinguality in, statistical world)\n",
      "(x, work with, pyspark)\n",
      "(zeppelin notebook, use, jupyter)\n",
      "(gcp, use, highmem instances)\n",
      "(memory config, set, x>64g)\n",
      "(data, cache, visualization)\n",
      "(modelling, reduce dimensions, columns/variables)\n",
      "(knowledge, is, beautiful)\n",
      "(knowledge, is, ambiguous)\n",
      "(knowledge, is, prone to misunderstandings)\n",
      "(I, wrote, a function)\n",
      "(function, translates, my former knowledge-DB)\n",
      "(function, generates, .md files)\n",
      "(For the future, I want to integrate, #rstudio)\n",
      "(For the future, I want to integrate, #wiki)\n",
      "(For the future, I want to integrate, #obsidian)\n",
      "(For the future, I want to integrate, #termstore)\n",
      "(For the future, I want to integrate, #sharepoint)\n",
      "(EMR R-Studio spark cluster, doesn't work, documentation)\n",
      "(EMR R-Studio spark cluster, build file, https://lnkd.in/gjR3iscZ)\n",
      "(spark, needs, { \"Classification\": \"spark\", \"Properties\": { \"maximizeResourceAllocation\": \"true\" } })\n",
      "(reactlog, is, approach)\n",
      "(reactlog, helps to get in to, nlp)\n",
      "(networkD3, is, library)\n",
      "(custom, Use Case, thx)\n",
      "(custom, To, PetitBiscuit)\n",
      "(custom, for making, good music)\n",
      "(custom, for providing, transcripts)\n",
      "(custom, I have to benchmark, next time)\n",
      "(custom, against, aws transcriber)\n",
      "(custom, and, gcp)\n"
     ]
    }
   ],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    [document2],\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    service_context=service_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1602a1dded66e0d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:56:51.261706384Z",
     "start_time": "2023-09-24T16:56:47.561198399Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-ngql in ./venv/lib/python3.11/site-packages (0.7.5)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (3.1)\n",
      "Requirement already satisfied: pyvis in ./venv/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: Jinja2 in ./venv/lib/python3.11/site-packages (from ipython-ngql) (3.1.2)\n",
      "Requirement already satisfied: nebula3-python>=3.4.0 in ./venv/lib/python3.11/site-packages (from ipython-ngql) (3.4.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from ipython-ngql) (2.1.1)\n",
      "Requirement already satisfied: ipython>=5.3.0 in ./venv/lib/python3.11/site-packages (from pyvis) (8.15.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in ./venv/lib/python3.11/site-packages (from pyvis) (3.0.2)\n",
      "Requirement already satisfied: backcall in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (5.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from Jinja2->ipython-ngql) (2.1.3)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in ./venv/lib/python3.11/site-packages (from nebula3-python>=3.4.0->ipython-ngql) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in ./venv/lib/python3.11/site-packages (from nebula3-python>=3.4.0->ipython-ngql) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in ./venv/lib/python3.11/site-packages (from nebula3-python>=3.4.0->ipython-ngql) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in ./venv/lib/python3.11/site-packages (from nebula3-python>=3.4.0->ipython-ngql) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas->ipython-ngql) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->ipython-ngql) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.11/site-packages (from pandas->ipython-ngql) (2023.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./venv/lib/python3.11/site-packages (from httplib2>=0.20.0->nebula3-python>=3.4.0->ipython-ngql) (3.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.11/site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.11/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.11/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Connection Pool Created\n",
      "INFO:nebula3.logger:Get connection to ('0.0.0.0', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo_data_lineage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linkedin_as_knowledge_graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linkedin_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llamaindex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name\n",
       "0            demo_data_lineage\n",
       "1  linkedin_as_knowledge_graph\n",
       "2                  linkedin_v2\n",
       "3                   llamaindex"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install related packages, password is nebula by default\n",
    "%pip install ipython-ngql networkx pyvis\n",
    "%load_ext ngql\n",
    "%ngql --address 0.0.0.0 --port 9669 --user root --password <password>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a93b2729f564105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:57:06.058428717Z",
     "start_time": "2023-09-24T16:57:05.927732409Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('0.0.0.0', 9669)\n",
      "INFO:nebula3.logger:Get connection to ('0.0.0.0', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\"@alanbecker\")-[:relationship@131303000615340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\"@prosperpython\")-[:relationship@131303000615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\"AI composing\")-[:relationship@-8161707173856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(\"Aldi\")-[:relationship@-6959414471673446724{r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(\"Aldi\")-[:relationship@-6959414471673446724{r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>(\"whisper api\")-[:relationship@201934314854289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>(\"whisper api\")-[:relationship@201934314854289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>(\"work\")-[:relationship@6158135741405254097{re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>(\"x\")-[:relationship@-232941839542300431{relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>(\"zeppelin notebook\")-[:relationship@-51640910...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     e\n",
       "0    (\"@alanbecker\")-[:relationship@131303000615340...\n",
       "1    (\"@prosperpython\")-[:relationship@131303000615...\n",
       "2    (\"AI composing\")-[:relationship@-8161707173856...\n",
       "3    (\"Aldi\")-[:relationship@-6959414471673446724{r...\n",
       "4    (\"Aldi\")-[:relationship@-6959414471673446724{r...\n",
       "..                                                 ...\n",
       "158  (\"whisper api\")-[:relationship@201934314854289...\n",
       "159  (\"whisper api\")-[:relationship@201934314854289...\n",
       "160  (\"work\")-[:relationship@6158135741405254097{re...\n",
       "161  (\"x\")-[:relationship@-232941839542300431{relat...\n",
       "162  (\"zeppelin notebook\")-[:relationship@-51640910...\n",
       "\n",
       "[163 rows x 1 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query some random Relationships with Cypher\n",
    "# %ngql USE llamaindex;\n",
    "%ngql USE linkedin_v2;\n",
    "%ngql MATCH ()-[e]->() RETURN e LIMIT 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bf12770cada0d52b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:57:17.540559606Z",
     "start_time": "2023-09-24T16:57:17.375600195Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"nebulagraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8b72746810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<class 'pyvis.network.Network'> |N|=214 |E|=163"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw the result\n",
    "\n",
    "%ng_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9efae7b5-ead9-447d-852e-20cd2a8c0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm=OpenAI(temperature=0, model_name=\"text-davinci-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e0611e9e7efbd233",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "query_engine = KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84b0d305-b059-49d7-b944-18448b7ebdc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33;1m\u001B[1;3mGraph Store Query:\n",
      "\n",
      "MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;\n",
      "\u001B[0mERROR:llama_index.graph_stores.nebulagraph:Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;, Param: {}Error message: Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;, Param: {}Error message: SemanticError: `person': Unknown tag\n",
      "ERROR:llama_index.graph_stores.nebulagraph:Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;, Param: {}Error message: Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;, Param: {}Error message: SemanticError: `person': Unknown tag\n",
      "ERROR:llama_index.graph_stores.nebulagraph:Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;, Param: {}Error message: Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
      "RETURN p.`person`.`name`;, Param: {}Error message: SemanticError: `person': Unknown tag\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x7f8b728d2ad0 state=finished raised ValueError>]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/llama_index/graph_stores/nebulagraph.py:300\u001B[0m, in \u001B[0;36mNebulaGraphStore.execute\u001B[0;34m(self, query, param_map)\u001B[0m\n\u001B[1;32m    296\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m    297\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuery failed. Query: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Param: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_map\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    298\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError message: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    299\u001B[0m     )\n\u001B[0;32m--> 300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;66;03m# other exceptions\u001B[39;00m\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/llama_index/graph_stores/nebulagraph.py:276\u001B[0m, in \u001B[0;36mNebulaGraphStore.execute\u001B[0;34m(self, query, param_map)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39mis_succeeded():\n\u001B[0;32m--> 276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuery failed. Query: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Param: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_map\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError message: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;241m.\u001B[39merror_msg()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    279\u001B[0m     )\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mValueError\u001B[0m: Query failed. Query: MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\nRETURN p.`person`.`name`;, Param: {}Error message: SemanticError: `person': Unknown tag",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRetryError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mquery_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhat do i say about math?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m display(Markdown(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<b>\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m</b>\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/llama_index/indices/query/base.py:23\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     22\u001B[0m     str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n\u001B[0;32m---> 23\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/llama_index/query_engine/knowledge_graph_query_engine.py:235\u001B[0m, in \u001B[0;36mKnowledgeGraphQueryEngine._query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Query the graph store.\"\"\"\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    233\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str}\n\u001B[1;32m    234\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[0;32m--> 235\u001B[0m     nodes: List[NodeWithScore] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_synthesizer\u001B[38;5;241m.\u001B[39msynthesize(\n\u001B[1;32m    238\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery_bundle,\n\u001B[1;32m    239\u001B[0m         nodes\u001B[38;5;241m=\u001B[39mnodes,\n\u001B[1;32m    240\u001B[0m     )\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verbose:\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/llama_index/query_engine/knowledge_graph_query_engine.py:200\u001B[0m, in \u001B[0;36mKnowledgeGraphQueryEngine._retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    193\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGraph Store Query:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mgraph_store_query\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    196\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mRETRIEVE,\n\u001B[1;32m    197\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: graph_store_query},\n\u001B[1;32m    198\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;66;03m# Get the graph store response\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m     graph_store_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_store\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_store_query\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verbose:\n\u001B[1;32m    202\u001B[0m         print_text(\n\u001B[1;32m    203\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGraph Store Response:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mgraph_store_response\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    204\u001B[0m             color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    205\u001B[0m         )\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/llama_index/graph_stores/nebulagraph.py:671\u001B[0m, in \u001B[0;36mNebulaGraphStore.query\u001B[0;34m(self, query, param_map)\u001B[0m\n\u001B[1;32m    670\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m, param_map: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m {}) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 671\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    672\u001B[0m     columns \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m    673\u001B[0m     d: Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mlist\u001B[39m] \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/Dokumente/Git/llm-index/venv/lib/python3.11/site-packages/tenacity/__init__.py:326\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreraise:\n\u001B[1;32m    325\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m retry_exc\u001B[38;5;241m.\u001B[39mreraise()\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfut\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexception\u001B[39;00m()\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait:\n\u001B[1;32m    329\u001B[0m     sleep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait(retry_state)\n",
      "\u001B[0;31mRetryError\u001B[0m: RetryError[<Future at 0x7f8b728d2ad0 state=finished raised ValueError>]"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"what do i say about math?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "211d06a5-d569-4a36-9ec6-3aa85ef9ed9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```cypher\n",
       "\n",
       "MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
       "RETURN p.`person`.`name`;\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_query = query_engine.generate_query(\n",
    "    \"math?\",\n",
    ")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b7cf626-e04b-4884-96f1-544faa915859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('0.0.0.0', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p.entity.name</th>\n",
       "      <th>e.relationship</th>\n",
       "      <th>m.entity.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math</td>\n",
       "      <td>is</td>\n",
       "      <td>beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math</td>\n",
       "      <td>is</td>\n",
       "      <td>enjoyable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math</td>\n",
       "      <td>is</td>\n",
       "      <td>remedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  p.entity.name e.relationship m.entity.name\n",
       "0          math             is     beautiful\n",
       "1          math             is     enjoyable\n",
       "2          math             is        remedy"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql \n",
    "MATCH (p:`entity`)-[e:relationship]->(m:`entity`)\n",
    "  WHERE p.`entity`.`name` == 'math'\n",
    "RETURN p.`entity`.`name`, e.relationship, m.`entity`.`name`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "74186452-9873-4cb3-92f1-61b9c1216c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"nebulagraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8b728786d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<class 'pyvis.network.Network'> |N|=0 |E|=0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ng_draw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
